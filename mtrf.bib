------------------------
Roleo 2016
------------------------

@inproceedings{van2014neural,
  title={A neural network approach to selectional preference acquisition},
  author={Van de Cruys, Tim},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={26--35},
  year={2014}
}

@InProceedings{konstas-keller:2015:ACL-IJCNLP,
  author    = {Konstas, Ioannis  and  Keller, Frank},
  title     = {Semantic Role Labeling Improves Incremental Parsing},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  month     = {July},
  year      = {2015},
  address   = {Beijing, China},
  publisher = {Association for Computational Linguistics},
  pages     = {1191--1201},
  url       = {http://www.aclweb.org/anthology/P15-1115}
}

@article{grissom2014don,
  title={Don’t Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation},
  author={Grissom II, Alvin C and Boyd-Graber, Jordan and Boyd, Jordan and He, He and Morgan, John and Daum{\'e} III, Hal},
  year={2014}
}

@article{tan2012scalable,
  title={A scalable distributed syntactic, semantic, and lexical language model},
  author={Tan, Ming and Zhou, Wenli and Zheng, Lei and Wang, Shaojun},
  journal={Computational Linguistics},
  volume={38},
  number={3},
  pages={631--671},
  year={2012},
  publisher={MIT Press}
}

@article{hitchcock1927expression,
  title={The expression of a tensor or a polyadic as a sum of products},
  journal={Journal of Mathematics and Physics},
  author={Hitchcock, Frank Lauren},
  year={1927},
  number={6},
  pages={164–-189}
}

@inproceedings{chelba1998exploiting,
  title={Exploiting syntactic structure for language modeling},
  author={Chelba, Ciprian and Jelinek, Frederick},
  booktitle={Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1},
  pages={225--231},
  year={1998},
  organization={Association for Computational Linguistics}
}


@article{klakow2002testing,
  title={Testing the correlation of word error rate and perplexity},
  author={Klakow, Dietrich and Peters, Jochen},
  journal={Speech Communication},
  volume={38},
  number={1},
  pages={19--28},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{kiros2014multimodal,
  title={Multimodal neural language models},
  author={Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Rich},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  pages={595--603},
  year={2014}
}
@inproceedings{alumae2013multi,
  title={Multi-domain neural network language model.},
  author={Alum{\"a}e, Tanel},
  booktitle={INTERSPEECH},
  pages={2182--2186},
  year={2013},
  organization={Citeseer}
}
@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  journal={The Journal of Machine Learning Research},
  volume={3},
  pages={1137--1155},
  year={2003},
  publisher={JMLR. org}
}
@inproceedings{sutskever2011generating,
  title={Generating text with recurrent neural networks},
  author={Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
  booktitle={Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  pages={1017--1024},
  year={2011}
}
@article{memisevic2010learning,
  title={Learning to represent spatial transformations with factored higher-order Boltzmann machines},
  author={Memisevic, Roland and Hinton, Geoffrey E},
  journal={Neural Computation},
  volume={22},
  number={6},
  pages={1473--1492},
  year={2010},
  publisher={MIT Press}
}
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={NATURE},
  volume={323},
  pages={9},
  year={1986}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={The Journal of Machine Learning Research},
  volume={12},
  pages={2121--2159},
  year={2011},
  publisher={JMLR. org}
}

@MISC{Bastien-Theano-2012,
        author = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
         title = {Theano: new features and speed improvements},
          year = {2012},
  howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},
      abstract = {Theano is a linear algebra compiler that optimizes a user’s symbolically-speciﬁed
mathematical computations to produce efﬁcient low-level implementations. In
this paper, we present new features and efﬁciency improvements to Theano, and
benchmarks demonstrating Theano’s performance relative to Torch7, a recently
introduced machine learning library, and to RNNLM, a C++ library targeted at
recurrent neural networks.}
}

@INPROCEEDINGS{bergstra+al:2010-scipy,
     author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
       note = {Oral Presentation},
   abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.}
}


@inproceedings{LenciECU,
 author = {Lenci, Alessandro},
 title = {Composing and Updating Verb Argument Expectations: A Distributional Semantic Model},
 booktitle = {Proceedings of the 2Nd Workshop on Cognitive Modeling and Computational Linguistics},
 series = {CMCL '11},
 year = {2011},
 location = {Portland, Oregon},
 pages = {58--66},
 numpages = {9},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 

@article{Binder2001,
   title={The effects of thematic fit and discourse context on syntactic 
ambiguity resolution},
   author={Binder, Katherine S and Duffy, Susan A and Rayner, Keith},
   journal={Journal of Memory and Language},
   volume={44},
   number={2},
   pages={297--324},
   year={2001},
   publisher={Elsevier}
}

