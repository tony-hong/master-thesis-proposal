\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{paralist}
\graphicspath{ {figs/} }


%% \usepackage[numbers]{natbib}

\title{Learning Representations of Semantic Role Fillers in Events with Neural Networks}
\author{Author: Xudong Hong \\ 
Supervisor: Asad Sayeed, Vera Demberg}

\begin{document}
\maketitle

\begin{abstract}
We introduce a neural network model capturing both lexical and semantic features of event participants. Possible methods to composite word and semantic role representations are discussed. Making use of tensor factorisation on role-specific word embedding \citep{tilk-EtAl:2016:EMNLP2016}, we represent the order-3 tensor feature space using 3 matrices. Then we add non-linearity and construct the model as a neural network. The model is trained in multitask style to counter both semantic role-filler prediction and semantic role-label prediction tasks. Comparing to previous works, the model has improvement on both semantic role/role-filler modelling and human thematic fit judgement correlations task. Moreover, the trained model has high accuracy on semantic role prediction. 
\end{abstract}



\section{Introduction}
Event participants can be mapped to arguments of a predicate.
A level of representation to show the common characteristics is \textit{semantic roles} which express the abstract role of the argument of a predicate can taken in an event. 
For example, 
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{BOP.png}
\caption{\label{fig:SemanticRoles} An event with predicate and event participants. Each event participant is annotated with its corresponding semantic role}
\end{figure}

\noindent


\noindent
Distributed representation of words is useful in natural language understanding, natural language generation and machine translation. There are many efforts on learning word embedding from large scale text corpora. Nevertheless for generality, most of these works only use raw word as input without using any semantic features. 


\noindent
The semantic role labeling task is to assign semantic role labels to the event participants in an event. After many years of research, pre-trained semantic role labeling systems are quite reliable. However, most of these works only assign labels to the whole constituents or phrases. \citet{Sayeed2014} proposed an unsupervised method of assigning semantic roles to headwords in event participants to construct a corpus. The raw text are firstly processed by a state-of-the-art semantic role labeller SENNA \citep{Collobert}. Then a dependency parser Malt Parser is used to extract the headwords of the event participants together with some heuristics.


\noindent
In this paper, we present a model ...



\section{Related Works}

\subsection{Representation Learning}

Words can be represented with vectors in adjacent matrix using count based method. But when it comes to the practice, this kind of method has a problem of sparsity. The vector of count-based method is discrete, high-dimensional and sparse. Distributed representation is an effective method to tackle sparsity of high-dimensional vectors. \\
\noindent
\citet{mikolov2013distributed} proposed recurrent neural network based language model.

\subsection{Semantic Role Labeling}
There are many state-of-the-art SRL systems \citep{collobert2011natural, titov2011bayesian}. \citet{roth2016neural} proposed PathLSTM which made use of syntactic features. But \citet{marcheggiani2017simple} claimed that syntactic features is not necessary for semantic role labeling.

\subsection{Cognitive Modelling}
\subsubsection{Event Level Model}
Event-level knowledge does affect human expectations for verbal arguments \citep{baroni2010distributional}. \\
\noindent 
Regarding to models of events, \citet{tilk2016event} proposed two models: \textit{non-incremental model} and \textit{incremental model}. For the \textit{non-incremental model}, they simply summed all the vectors $p$ of event participants together to represent the event $e$ as a vector:
\begin{equation} \label{eq:20}
\begin{aligned}
    \mathbf{e}
        &= \sum_{l=1}^{L} \mathbf{p}_{l} \\
\end{aligned}
\end{equation}
where $L$ is the number of participants in event $e$. \\
\noindent
When it comes to the \textit{incremental model}, they added a recurrent unit to express participant order not only within event but also across different events. Moreover, a binary indicator $b$ is used to detect the event boundaries. $b$ equals to $1$ if the target word belongs to current event, otherwise $0$. So the event representation vector is: 
\begin{equation} \label{eq:21}
\begin{aligned}
    \mathbf{e}
        &= \mathbf{p} + \mathbf{h_{t-1}}\mathbf{W}_h + b\mathbf{v}
\end{aligned}
\end{equation} 
where $\mathbf{v}$ is the parameter vector of event boundary. $\mathbf{h_{t-1}}$ is the hidden vector of last time step in the recurrent unit. $\mathbf{W}_h$ is the parameter matrix of recurrent unit. 

\subsubsection{Thematic Fit Evaluation}
\citet{sayeed2014combining} 
Extract thematic features using semantic role labeler SENNA \citep{collobert2011natural}.
Construct distributional memory model. 



\section{Representation of Event Participants}
\subsection{Event Participant Embedding}
We intend to learn meaningful representation in semantic space of words and roles. Firstly we define the symbolic representation for words. Word $a_i$ is encoded as one-hot row vector $\mathbf{w}_i = [00...1...0]$ containing exactly one $1$ value in the position of word index $i$ and other values are all $0$, where $|\mathbf{w}_i| = |V|$ and $V$ is the word vocabulary. In this vector space, words are mutual exclusive to each other.  \\
\noindent
Futher than that, we need to obtain a distributed representation that is smoother. So we compute the inner product between $\mathbf{w}_i$ and a embedding matrix $\mathbf{A} \in \mathbb{R}^{|V| \times k}$ as $\mathbf{A}_{(i)} = \mathbf{w}_i \mathbf{A}$. The vector $\mathbf{w}_i$ is now mapped to low-dimensional vector $\mathbf{A}_{(i)} \in \mathbb{R}^k$ which is the $i$-th row of matrix $\mathbf{A}$. The symbolic representations of words are transformed into distributed representations. \\
\noindent
In addition to word embedding, we want to learn the representation of words given a specific semantic role. Performing matricization on a order-3 tensor which contains word-role-word tuple onto a matrix can obtain vectors from different semantic spaces \citep{baroni2010distributional}. Inspiring by this idea, we add one more dimension of semantic role to the word embedding matrix and obtain a third-order tensor $\mathbf{T} \in \mathbb{R}^{|V| \times |R| \times d}$. For each word $\mathbf{w}_i$ and each semantic role $\mathbf{r}_j$, there is a corresponding vector $\mathbf{T}_{(ij)}$ with length of $d$ where $i \in [1, |V|]$ and $j \in [1, |R|]$. This vector is named as \textit{role-specific word embedding} vector \citep{tilk2016event} which can be further simplified to \textbf{event participant embedding} vector.



\subsection{Tensor Decomposition}
Although the role-specific word embedding is a versatile representation, it suffer the curse of dimensionality. There are $|V| \times |R| \times d$ parameters in the order-3 tensor $\mathbf{T}$. Since word vocabulary size $|V|$ is often very big (more than $50,000$), the number of parameters in the tensor is too huge, which makes the model hard to converge. \todo[]{more drawbacks} \\
\noindent
\citet{tilk2016event} proposed an effective tensor factorisation method on the parameter tensor to reduce the number of parameters. Similarly, we perform tensor rank decomposition \citep{hitchcock1927expression} on the embedding tensor in real space:
\begin{equation} \label{eq:trd}
\begin{aligned}
    \mathbf{T}   
        &= \sum_{m=1}^{k} \mathbf{T}'_m \\
        &= \sum_{m=1}^{k} \mathbf{a}_m \otimes \mathbf{b}_m \otimes \mathbf{c}_m, \\
\end{aligned}
\end{equation}
where tensor $\mathbf{T}$ is decomposed into $k$ rank-1 tensor $\mathbf{T}'$. Each rank-1 tensor can be written as outer product of 3 row vectors $\mathbf{a}_m$, $\mathbf{b}_m$ and $\mathbf{d}_m$. The smallest $k$ makes the equation true is the tensor rank of $\mathbf{T}$. We then group $k$ vectors $\mathbf{a}_m$, $\mathbf{b}_m$ by column into matrices $\mathbf{A} \in \mathbb{R}^{|V| \times k}$, $\mathbf{B} \in \mathbb{R}^{|R| \times k}$ and $\mathbf{c}_m$ by row into matrices $\mathbf{C} \in \mathbb{R}^{k \times d}$. Now the number of parameters is reduced to $(|V| + |R| + d) \times k$ which is comparable smaller. \\
\noindent
Naturally, matrix $\mathbf{A}$ can be interpreted as word embedding described above. Similar to word embedding, we can represent semantic role $b_j$ using one-hot encoded column vector $\mathbf{r}_j$ where $|\mathbf{r}_j| = |R|$ and $R$ is the set of semantic roles. So $\mathbf{B}$ can be considered as role embedding matrix. By computing inner product between $\mathbf{r}_j$ and $\mathbf{B}$ as $\mathbf{B}_{(j)} = \mathbf{r}_j \mathbf{B}$, we obtain the low-dimensional vector $\mathbf{B}_{(j)} \in \mathbb{R}^k$ which is the $j$-th row of matrix $\mathbf{B}$. From equation \eqref{eq:trd}, the vector $\mathbf{T}_{(ij)}$ can be computed as: 
\begin{equation} \label{eq:rbwe_tensor}
\begin{aligned}
    \mathbf{T}_{(ij)}
        &= \sum_{m=1}^{k} \mathbf{a}_{m_{(i)}} \otimes \mathbf{b}_{m_{(j)}} \otimes \mathbf{c}_m \\
        &= \sum_{m=1}^{k} \mathbf{a}_{m_{(i)}} \mathbf{b}_{m_{(j)}}  \mathbf{c}_m \\
\end{aligned}
\end{equation}
\noindent
To get rid of the outer product and make it easier to implement, we use the decomposition matrices defined above. Thus the vector $\mathbf{T}_{(ij)}$ can be written as a vector: 
\begin{equation} \label{eq:rbwe}
\begin{aligned}
    \mathbf{p} 
        &= (\mathbf{A}_{(i)}\circ \mathbf{B}_{(j)}) \mathbf{C} \\
        &= (\mathbf{w}_i \mathbf{A} \circ \mathbf{r}_j \mathbf{B}) \mathbf{C} \\
\end{aligned}
\end{equation}
which is a representation of a participant in an event. "$\circ$" is Hadamard product which computes element-wise multiplication between two vectors. This can be interpreted as the composition method of word embedding vector and role embedding vector. 



\begin{figure}[t]
\centering
\includegraphics[width=0.65\textwidth]{NNRF.png}
\caption{\label{fig:NNRF} Model architecture of non-incremental role filler.}
\end{figure}



\section{Model Design}
To tackle the role filler prediction task, \citet{tilk2016event} proposed the \textit{non-incremental role filler}\textbf{(NNRF)}, a 2-layer neural network model. As in Figure \ref{fig:NNRF}, from input to output, the model uses role based word representations of event participants as input. Parameters are learned together with the model. The embedding vectors are composed using sum method into an representation of event participants except the target participant. Then it passes through one non-linearity layer with parametric rectified linear unit. After that, a softmax regression classifier is used for prediction. 


\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{MTNNRF.png}
\caption{\label{fig:MTNNRF} Model architecture of multitask version of non-incremental role filler.}
\end{figure}


In the \textbf{NNRF} model, embedding vectors of event participants are sum up together to represent the whole event. Nonetheless, this is only a simple method to composite event participants. Each participant has same weight in the event. Additionally, means of parameters can shift in a large range the after the sum operation. How can we improve this? We need an exploration of composition methods. \\
We separate all possible methods in two groups. The first group is \textbf{non-parametric methods} which have no extra parameters like addition, multiplication and concatenation of event participant vectors. Another group is \textbf{parametric methods} which need more parameters like recursive unit \citep{socher2013recursive}, recurrent unit \citet{mikolov2010recurrent}, or long-short-term-memory recurrent unit \citep{hochreiter1997LSTM}. Considering the sequence of participants, there are two different conditions: the participant order is unknown and the opposite case. In this work, we focus on non-parametric methods in the condition that participant order is not provided and leave other cases for future exploration. \\
\noindent
Multiplication is an effective composition method for distributed memory models. However, during optimisation of neural network models, multiplication often results in extreme values which will lead to overflow or underflow. Concatenation is widely used in language modelling. But usually the concatenated feature can not be used directly. An extra feature extractor is needed, which turns it into a parametric method. \\ 
\noindent
We propose the \textbf{mean} composition method which compute the mean of all the  vectors of event participants to represent the event as one vector. Computing mean instead of sum can be considered as normalisation of participant representations within the event boundary, which can prevent possible overflow/underflow of the hidden values. \\
Since it is a non-parametric method, we also want to add a weight before the mean composition. Instead of using a fully-connected layer, we apply the PReLU to each embedding vector of participant here. So the parameters inside PReLU can also act as weights of each participant. 
\noindent
This model is described in Figure \ref{fig:MTRFv4}. 
\begin{equation} \label{eq:nonlinearity}
\begin{aligned}
    \mathbf{h}_l
        &= PReLU_l(\mathbf{p}_l) \\
\end{aligned}
\end{equation}
\noindent
Using the mean method, the embedding vectors are composed as:
\begin{equation} \label{eq:mean_comp}
\begin{aligned}
    \mathbf{e}
        &= \frac{1}{L} \sum_{l=1}^{L} \mathbf{h}_{l} \\
\end{aligned}
\end{equation}



\subsection{Multitask Learning}
The \textbf{NNRF} model is trained to solve the role filler prediction task. When we want to obtain the representation of the event, there is a problem if we only focus on role filler prediction. The representations with same arguments but different roles can be very similar to each other. For example, the embedding vector of $(boy_{ARG0}, eat_{PREDICATE}, apple_{ARG1})$ can be very similar to the embedding vector of  $(apple_{ARG0}, eat_{PREDICATE}, boy_{ARG1})$. \\
To tackle this problem we add one more auxiliary task which is  \textbf{semantic role prediction} task:
\begin{itemize}
  \item  predict the target semantic role given the corresponding target role role filler and the other event participants  in an event.
\end{itemize}
This task can be considered as a regularisation of the objective function of the role filler prediction task.
\noindent
Since we want to obtain the probability distribution of target word or target role, in the last layer a softmax regression classifier is used for each task. The output vectors of these classifiers are computed as:
\begin{equation} \label{eq:output}
\begin{aligned}
    \mathbf{o}_w
        &= \mathbf{e}\mathbf{W}_w + \mathbf{b}_w \\
    \mathbf{o}_r
        &= \mathbf{e}\mathbf{W}_r + \mathbf{b}_r \\
\end{aligned}
\end{equation}
where $\mathbf{W}_w \in \mathbb{R}^{d \times |V|}$ is a weight matrix of the target word classifier,  $\mathbf{W}_r \in \mathbb{R}^{d \times |R|}$ is a weight matrix of the target role classifier and $\mathbf{b}_w$, $\mathbf{b}_r$ are the corresponding biases of the classifier. The conditional probability of target word $a_x$ given event $e$ and target role $b_y$ is:
\begin{equation} \label{eq:softmax_w}
\begin{aligned}
    p(a_x | e, b_y)
        &= Softmax(\mathbf{o}_w)_{(x)} \\
        &= \frac{
        exp(\mathbf{o}_w)_{(x)}
        }{
        \sum_{i=1}^{|V|} exp(\mathbf{o}_w)_{(i)} }   \\
\end{aligned}
\end{equation}
where suffix $(x)$ is the $x$-th element of the vector. And the conditional probability of target role $b_y$ given event $e$ and target word $a_x$ is:
\begin{equation} \label{eq:softmax_r}
\begin{aligned}
    p(b_y | e, a_x)
        &= Softmax(\mathbf{o}_r)_{(y)} \\
        &= \frac{
        exp(\mathbf{o}_r)_{(y)}
        }{
        \sum_{j=1}^{|R|} exp(\mathbf{o}_r)_{(j)} }   \\
\end{aligned}
\end{equation}



\subsection{Target Role Specific Classifier}
For each target role or target word, we need a different weight matrix to perform classification. How can we integrate the information of target role or target word with two weight matrices in an fully interpretable way without any additional parameters? We perform a similar technique like the role-specific word embedding. \\
At first we define weight matrix $\mathbf{W}_w$ and weight matrix $\mathbf{W}_r$ for each target word $a_x$ and target role $b_y$. Then we stack $\mathbf{W}_w$ and $\mathbf{W}_r$ from $a_1$ to $a_{|V|}$ and from $b_1$ to $b_{|R|}$. Two weight tensors $\mathbf{T}^{(w)} \in \mathbb{R}^{d \times |R| \times |V|}$ and $\mathbf{T}^{(r)} \in \mathbb{R}^{d \times |V| \times |R|}$ are obtained. But these tensors suffer the same dimensionality problem as role based embedding tensor. So we apply tensor rank decomposition again on two weight tensors, and extract the corresponding weight matrices for $a_x$ and $b_y$ as follow: 
\begin{equation} \label{eq:trd_cls}
\begin{aligned}
    \mathbf{T}_{(x)}^{(w)}
        &= \sum_{m=1}^{k^{(w)}} \mathbf{c}_{m}^{(w)} \otimes \mathbf{b}_{m_{(x)}}^{(w)} \otimes \mathbf{a}_m^{(w)} \\
    \mathbf{T}_{(y)}^{(r)}
        &= \sum_{m=1}^{k^{(r)}} \mathbf{c}_{m}^{(r)} \otimes  \mathbf{a}_{m_{(y)}}^{(r)} \otimes \mathbf{b}_m^{(r)} \\
\end{aligned}
\end{equation}
Similarly, we group vectors $\mathbf{c}_{m}^{(w)}$, $\mathbf{c}_{m}^{(r)}$ by row into matrices $\mathbf{C}_w \in \mathbb{R}^{d \times k^{(w)}}$, $\mathbf{C}_r \in \mathbb{R}^{d \times k^{(r)}}$ and group $\mathbf{a}_m^{(w)}$, $\mathbf{b}_m^{(r)}$ by column into matrices $\mathbf{A}_w \in \mathbb{R}^{k^{(w)} \times |V|}$, $\mathbf{B}_r \in \mathbb{R}^{k^{(r)} \times |R|}$. After getting rid of outer product using same transformation as equation \eqref{eq:rbwe}, two weight matrices in equation \eqref{eq:output}  can be written as: 
\begin{equation} \label{eq:cls}
\begin{aligned}
    \mathbf{W}_w
        &= \mathbf{C}_w \ diag(\mathbf{b}_y) \ \mathbf{A}_w \\
    \mathbf{W}_r
        &= \mathbf{C}_r \ diag(\mathbf{a}_x) \ \mathbf{B}_r \\
\end{aligned}
\end{equation}
where $diag(\mathbf{v})$ is a diagonal matrix with vector $\mathbf{v}$ on its diagonal. Since there is a vector $\mathbf{b}_y$ representing target role $b_y$ and there is a vector $\mathbf{a}_x$ representing target word $a_x$. Now we can define the embedding matrices $\mathbf{B}_w \in \mathbb{R}^{k^{(w)} \times |R|}$, $\mathbf{A}_r \in \mathbb{R}^{k^{(r)} \times |V|}$ for target roles and target words. The weight matrices can be further written as: 
\begin{equation} \label{eq:cls_temb}
\begin{aligned}
    \mathbf{W}_w
        &= \mathbf{C}_w \ diag(\mathbf{r}_y \mathbf{B}_w) \ \mathbf{A}_w \\
    \mathbf{W}_r
        &= \mathbf{C}_r \ diag(\mathbf{w}_x \mathbf{A}_r) \ \mathbf{B}_r \\
\end{aligned}
\end{equation}



\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{BOP.png}
\caption{\label{fig:BOP} Model architecture of multitask role filler.}
\end{figure}




\section{Evaluation}

\subsection{Human thematic fit judgement correlation task}







\section{Conclusion}





\newpage



\section{Some examples to get started}

\subsection{How to include Figures}

First you have to upload the image file from your computer using the upload link the project menu. Then use the includegraphics command to include it in your document. Use the figure environment and the caption command to add a number and a caption to your figure. See the code for Figure \ref{fig:frog} in this section for an example.

\begin{figure}
\centering
\includegraphics[width=0.3\textwidth]{frog.jpg}
\caption{\label{fig:frog}This frog was uploaded via the project menu.}
\end{figure}

\subsection{How to add Comments}

Comments can be added to your project by clicking on the comment icon in the toolbar above. % * <john.hammersley@gmail.com> 2016-07-03T09:54:16.211Z:
%
% Here's an example comment!
%
To reply to a comment, simply click the reply button in the lower right corner of the comment, and you can close them when you're done.

Comments can also be added to the margins of the compiled PDF using the todo command\todo{Here's a comment in the margin!}, as shown in the example on the right. You can also add inline comments:

\todo[inline, color=green!40]{This is an inline comment.}

\subsection{How to add Tables}

Use the table and tabular commands for basic tables --- see Table~\ref{tab:widgets}, for example. 

\begin{table}
\centering
\begin{tabular}{l|r}
Item & Quantity \\\hline
Widgets & 42 \\
Gadgets & 13
\end{tabular}
\caption{\label{tab:widgets}An example table.}
\end{table}

\subsection{How to write Mathematics}

\LaTeX{} is great at typesetting mathematics. Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
\[S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
      = \frac{1}{n}\sum_{i}^{n} X_i\]
denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.


\subsection{How to create Sections and Subsections}

Use section and subsections to organize your document. Simply use the section and subsection buttons in the toolbar to create them, and we'll handle all the formatting and numbering automatically.

\subsection{How to add Lists}

You can make lists with automatic numbering \dots

\begin{enumerate}
\item Like this,
\item and like this.
\end{enumerate}
\dots or bullet points \dots
\begin{itemize}
\item Like this,
\item and like this.
\end{itemize}

\subsection{How to add Citations and a References List}

You can upload a \verb|.bib| file containing your BibTeX entries, created with JabRef; or import your \href{https://www.overleaf.com/blog/184}{Mendeley}, CiteULike or Zotero library as a \verb|.bib| file. You can then cite entries from it, like this: \cite{greenwade93}. Just remember to specify a bibliography style, as well as the filename of the \verb|.bib|.

You can find a \href{https://www.overleaf.com/help/97-how-to-include-a-bibliography-using-bibtex}{video tutorial here} to learn more about BibTeX.

We hope you find Overleaf useful, and please let us know if you have any feedback using the help menu above --- or use the contact form at \url{https://www.overleaf.com/contact}!


\bibliographystyle{acl_natbib}
\bibliography{mtrf}


\end{document}