\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{minted}
\graphicspath{ {figs/} }

%% \usepackage[numbers]{natbib}


\title{Multi-task Learning for Event-Participant Embeddings}
\author{Author: Xudong Hong \\ 
Supervisors: Asad Sayeed, Vera Demberg}


\begin{document}
\maketitle


\begin{abstract}
\noindent
We introduce several neural network models to learn embeddings containing both lexical and semantic features of events and event participants. Possible methods to compose word and semantic role representations are discussed. Making use of tensor factorisation on role-specific word embedding \citep{tilk2016event}, we represent the order-3 tensor with three matrices. Then we add non-linearity and construct the model as a neural network. The model is trained in multi-task style to perform both semantic role-filler prediction and semantic role prediction. Comparing to previous work, the model has improved on both semantic role/role-filler modelling and human thematic fit judgement correlations task. Moreover, the trained model has high accuracy on semantic role prediction. The high-level features of the model can be used in incremental semantic role labelling.  
\end{abstract}



\section{Introduction} \label{sec:intro}
In natural language understanding, the representation of events and their participants takes an important role. To understand different sentences, we need to focus on event-level information among them  (see Section \ref{sec:event}). For example, in sentences like: 
\begin{eqnarray}
    & &\text{The cook cut the cake with knife in the kitchen.}        \label{eg:sent1} \\   
    & &\text{In the kitchen, the cake is cut with knife by the cook.} \label{eg:sent2}
\end{eqnarray}
the event-level information is actually the same. The event is described by the predicate $cut$ and the participants of this event are $cook$, $cake$, $knife$ and $kitchen$. Although the syntactic roles of participant $cook$ in two sentences are different (subject in sentence \eqref{eg:sent1} and object in sentence \eqref{eg:sent2}), they are both the $cutter$ of the $cake$. This shared information can be capture by a level of shallow semantic representation: \textit{semantic roles} \citep{jurafsky2014speech}. 

Semantic roles express the abstract roles of a participant can take in an event. In this level of representation, event participants can be mapped to arguments where they located. Using \textit{thematic role}, an inventory of conceptual argument roles (see Section \ref{sec:semanticrole}), the two sentences in earlier example can be annotated as: 
\begin{eqnarray} \label{eg:thematic}
    \nonumber & &[_{\texttt{AGENT}} \text{The }\textbf{cook}]\ cut\ [_{\texttt{PATIENT}}\text{the }\textbf{cake}]\ [_{\texttt{INSTRUMENT}}\text{with }\textbf{knife}]\\  
    &   &[_{\texttt{LOCATION}}\text{in the }\textbf{kitchen}]. \\
    \nonumber & &[_{\texttt{LOCATION }}\text{In the }\textbf{kitchen}],\ [_{\texttt{PATIENT }}\text{the }\textbf{cake }]\ \text{is} \ cut\ \\
    &   &[_{\texttt{INSTRUMENT }}\text{with }\textbf{knife}]\ [_{\texttt{AGENT }}\text{by the }\textbf{cook}]. 
\end{eqnarray} 
where the event participants are actually head nouns or noun-noun compounds inside arguments of the predicate $cut$. $The\ cook$, volitional causer of the event, has a \texttt{AGENT} role. And $the\ cake$, the participant mostly affected by the event, has a \texttt{PATIENT} role. But it is hard to define thematic role formally. As a result, the PropBank \citep{palmer2005proposition} style semantic role model is introduced where each semantic role is associated with a specific verb sense. So the representation of two sentences can be unified as: 
\begin{equation*} \label{eg:probank}
\begin{aligned}
    & \text{Predicate: }&&cut.\textbf{01} \\
    & \text{Arguments: }&&[_{\texttt{ARG0 }}\text{The }\textbf{cook}], \ [_{\texttt{ARG1 }}\text{the }\textbf{cake}], \ [_{\texttt{ARGM-MNR }}\text{with a }\textbf{knife}], \\
    &                   &&[_{\texttt{ARGM-LOC }}\text{in the }\textbf{kitchen}]
\end{aligned}
\end{equation*} 
where each argument is assigned with a semantic role label. $cut.01$ is the verbal predicate $cut$ with sense $01$ which is \textit{"make an incision or separation"}. Now the \texttt{ARG0} role can be defined exactly as \textit{"the subject which makes incision"}. To obtain this representation from raw text, one needs to solve semantic role labelling task (see Section \ref{sec:srl}). 

Since both raw words and semantic role labels are symbols, it has limited representing power. If someone wants to compare events quantitatively, it is not possible to use the \textit{symbolic representation} where each symbol represents a single state of the entry. There are some previous works construct numerical representation of linguistic units e.g. \textit{Distributional semantic model} (DSM) (see Section \ref{sec:dsm}). Among them, \textit{Distributed representation} of words, also named as \textit{word embedding}, is a well-established technique based on continuous feature space. Each word is represented by a low-dimensional dense vector with real number parameters which are learnt during training of a prediction task given the context. There are many efforts on learning word embeddings from large-scale corpora (see Section \ref{sec:repr}). 


\begin{table}[t]
\centering
\begin{tabular}{lll}
\textbf{Target Event}    &   \textbf{Foil Event}    &   \textbf{Similarity} \\  \hline
(machine, run, program)      &   (program, run, machine)     &   0.96    \\
(mother, supply, baby)   &   (baby, supply, mother)  &   0.95    \\
(firm, buy, politician)    &   (politician, buy, firm)   &   0.91    \\
\end{tabular}
\caption{\label{tab:eventsim} Some examples of similarity scores between two events using event embedding vectors in NNRF model. The metric is cosine similarity. }
\end{table}

\subsection{Main Problem and Previous Work} \label{sec:previous}
The main problem is that only a few works learn embeddings at the level of events. \citet{tilk2016event} proposed a model to learn a representation of events and their participants. To obtain a probability distribution over \textit{role-fillers} given event information, the models aim to perform \textit{role-filler prediction}: in each event, given the role label of one target word, and also given all other words and their role labels, the task is to predict the target word to fill the target role. The verbal predicate is considered as a special role $verb$ which is not in PropBank style semantic roles. The representation, named as \textit{role-specific word embedding}, learnt by those models is organised into an order-3 tensor. This means that for each word and each role, there is an embedding vector. To reduce the number of parameters, they factored the tensor into three matrices. 

However, there are some limitations in their work. Firstly, there is a problem of \textbf{event embeddings similarity}: the event embeddings with same argument words but different roles can be very similar to each other. For example, considering a target event $([_{\texttt{ARG0}} \text{investor}], buy, [_{\texttt{ARG1}}\text{share}])$ and the foil event $([_{\texttt{ARG0}}\text{share}], buy, [_{\texttt{ARG1}}\text{investor}])$, the model shows high similarity score between them. The foil event is unlikely to happen in real world, so human has a low similarity judgement score between it and the target event. But the model has a high similarity judgement score. So in this condition, their model is incompatible with human cognition. This is due to the smoothing effect of distributed representation. Their event embeddings are simply constructed as the sum of role-filler embeddings in each event. For events with a same set of words but different roles, features that characterize the event can be cancelled out during the composition of role-fillers. More examples are shown in Table \ref{tab:eventsim}. 
% * <asayeed@mbl.ca> 2017-11-13T22:30:35.273Z:
% 
% "based" event? ("foil" is pretty clear)
% 
% ^ <xhong@coli.uni-saarland.de> 2017-11-16T12:54:58.718Z:
% 
% Changed to 'original event'
%
% ^ <xhong@coli.uni-saarland.de> 2017-11-20T10:14:13.371Z:
% 
% Changed target event
%
% ^.

In addition to that, although the tensor-like structure of embedding is efficient and flexible, the training of the models are burdened by the gradient explosion or vanishing problem. Because in a factored order-3 tensor, each parameter is represented by a product of three parameters. To tackle this problem from the training aspect, they used Adagrad \citep{duchi2011adaptive} which adjusts learning rate during training. Although it alleviated the problem, it is not a solution on model architecture aspect. 

Furthermore, the factored order-3 tensor structure is not fully explained. Along with the rapid growth of architecture complexity, explainability of models is becoming more and more important. Yet they did not give a thorough explanation of the word embedding they obtained. Also there is no evaluation of their word embedding. Last but not least, the definition of the notion \textit{role} is vague, though they have a special role $verb$ for verbal predicates. This leads to an ambiguous definition of \textit{role-filler} which originally means the argument to fill one specific semantic role. 
% * <asayeed@mbl.ca> 2017-11-13T22:29:40.729Z:
% 
% Well I don't know if they're "major" disadvantages :)
% 
% ^ <xhong@coli.uni-saarland.de> 2017-11-16T12:58:14.475Z:
% 
% Could you please give a more specific comment? Do you mean that I should change it to ‘limitation'? 
% 
% ^ <asayeed@mbl.ca> 2017-11-16T21:52:11.087Z:
% 
% Yes, that is a better word.
%
% ^ <xhong@coli.uni-saarland.de> 2017-11-17T15:10:36.118Z.


\subsection{Main Goals} \label{sec:tasks}
In this paper, we aim to to tackle the event embeddings similarity problem and obtain an explainable representation of inner event structure with higher quality. In order to tackle the event embeddings similarity problem stated above, we need to train a model to discriminate different roles given same set of words. Besides we want to remain the smoothness of distributed representation of event. For these purposes, we design a training task which is: 
\begin{itemize}
  \item  \textbf{Semantic role prediction}: in each event, given one target word, and all other words and their semantic role labels, the task is to predict the target semantic role label. 
\end{itemize}
where we define \textbf{semantic roles} as a set which contains semantic roles in PropBank style for event participants and a special semantic role for predicates to get rid of ambiguity. We believe that by forcing the models to solve semantic role prediction during training, the event embeddings with same words but different roles will be distinguished. 

It has been shown that priming effect can be found in the directions of both predicate-to-participant and participant-to-predicate (see Section \ref{sec:event}). Since lexical features of participant and predicate are homogeneous, we can consider each of them as a word. With a view to the explainability of the task, we separate \textit{role-filler prediction} into two sub-tasks: 
\begin{itemize}
  \item  \textbf{Participant prediction}: in each event, given the predicate, the semantic role label of one target participant, the other participants and their semantic role labels, the task is to predict the target participant to fill the semantic role. 
  \item  \textbf{Predicate prediction}: in each event, given all participants and their semantic role labels, the task is to predict the predicate. 
\end{itemize}

Although we have three different tasks, multi-task learning is an effective approach to learn a join model to solve them at the same time \citep{caruana1998multitask}. By learning three tasks in parallel with a shared embedding of event predicates and participants, we can obtain a model to achieve our main goals. 


\subsection{Our Approaches} \label{sec:approaches}
We present several multi-task neural network models for both predicate prediction, participant prediction and semantic role prediction. To obtain an explainable event level embedding, we define the \textbf{event-participant embedding (EPE)} formally as the joint embedding of predicates that denote events and participants of events. The embedding vectors are stacked together to construct an order-3 tensor where each vector is indexed by a 2-tuple $(word,\ semantic\ role)$. As a result, each word can be represented as a matrix, indexed by a 1-tuple $(word)$, which can be considered as word embedding (see Section \ref{sec:epe}). To handle the curse of dimensionality, we apply tensor factorisation on the event-participant embedding tensor, which represents the tensor with three matrices. After detailed analysis in the tensor space, we demonstrate that these three matrices can be interpreted as word embedding, semantic role embedding and weight of linear mapping correspondingly (see Section \ref{sec:tf}). 

To extract event level information from text and to compare with previous baselines, we use a large scale corpus RW-eng providing access to generalized event knowledge (see Section \ref{sec:corpus}). Using the shared event-participant embedding as input, we add non-linearity layers to the model to construct a multi-task neural network. We design two classifiers with their own loss functions. One is for predicate prediction and participant prediction. The other one is for semantic role prediction. The models, named \textbf{multi-task event-participant (MTEP)} model, are optimised with different weight of each task. This model alleviate the event embeddings similarity problem and has better performance on event similarity task (see Section \ref{sec:mtep}). To improve the composition method and reduce its effect on event embeddings similarity problem, we discuss on several possible composition methods. We present an effective method to compose the event-level embedding using features from the hidden layer and design the \textbf{bag-of-predicate-participants (BOP)} model. The new composition method results in a better thematic fit model (see Section \ref{sec:bop}). To handle the gradient explosion/vanishing problem in the training of factored tensor structure, we integrate the residual learning framework into our model. We add one skip connection inside the factor tensor to construct a residual block and design the \textbf{residual bag-of-predicate-participants (ResBOP)} model (see Section \ref{sec:residual}). This method further improve the performance of the model. 


\subsection{Experiments and Evaluations} \label{sec:eval-steps}
We tune hyper-parameters to optimise for different objectives (see Section \ref{sec:exp}). On both three tasks, we compute the perplexity for each model(see Section \ref{sec:roleprediction}, \ref{sec:wordprediction}). The models are evaluated on human thematic fit judgement correlation task (see Section \ref{sec:thematicfit}). The event embeddings are evaluated on an event similarity correlation task (see Section \ref{sec:eventsim}). Last but not least, the word embeddings are evaluated on several word similarity tasks (see Section \ref{sec:wordsim}). The results show that our models have significant improvement over the current best system on specific evaluations while has competitive performance on the other evaluations. The event embedding extracted from the event-participant embedding we learnt also has state-of-the-art performance on event similarity correlation task. 
% * <asayeed@mbl.ca> 2017-11-13T22:34:00.165Z:
% 
% This is a very thorough high-level overview, but it's easy to get lost.  Maybe put in some subsections or bullet points?
% 
% ^ <xhong@coli.uni-saarland.de> 2017-11-16T12:59:10.662Z:
% 
% Is it only the paragraph above the comment or all three about contribution? 
%
% ^ <asayeed@mbl.ca> 2017-11-16T21:53:28.466Z:
% 
% There are parts of the above three paragraphs that look like "steps" and some that read like "reasons".  I think they could be reorganized so that we can follow all the moving parts.  So yeah, all three.
%
% ^.


\subsection{Contribution} \label{sec:contribution}
From the Cognitive modelling perspective, our models are effective models on both thematic fit modelling and semantic role prediction. The model can be consider as a surprisal model of semantic roles at the event level. The model can also generate a probability distribution over selection preferences given predicate and semantic roles, which can be use to compute event-level word surprisal. 

When it comes to the application perspective, our best model is a state-of-the-art system on several human thematic fit judgement correlation tasks, which can evaluate selectional preferences on predicate-argument pairs. The selectional preference features we obtain are useful for referent prediction in discourse-level \citep{modi2017modeling}. In simultaneous machine translation between language pairs with different verb position, adding predicate prediction leads to better translations \citep{grissom2014don}. The event embedding we learn can be used in scripts learning and paraphrase detection. The composite embedding of predicate and target head word can be used for argument classification in semantic role labelling \citep{roth2016neural}. The word embedding we learn focuses on content words which can be further employed in word sense disambiguation, discourse relation classification \citep{shi2017need, rutherford2017systematic}, sentiment analysis and natural language generation. 


 
\section{Background}


\begin{table}[t]
\centering
\begin{tabular}{l|l}
\textbf{Thematic Role}  &   \textbf{Definition} \\ \hline
\texttt{AGENT}                      &   The volitional causer of an event \\
\texttt{PATIENT} (\texttt{THEME})   &   The participant most directly affected by an event \\
\texttt{INSTRUMENT}                 &   The inanimate force or object used in an event \\
\texttt{LOCATION}                   &   The location or spatial orientation of the event \\
\end{tabular}
\caption{\label{tab:thematic} Definitions of common thematic roles.}
\end{table}


\subsection{Event Knowledge} \label{sec:event}
In psycholinguistics and cognitive science, experiments have shown that event knowledge plays a crucial role in human sentence processing \citep{camblin2007interplay}. On one hand, predicates prime event participants referring to good fillers of their semantic role in an event \citep{ferretti2001integrating}. On the other hand, priming effect can be also found from event participants to predicates \citep{mcrae2005basis}. These two works reveal that model performances of predicate prediction and participant prediction can be evaluated via correlation with human judgement. 

In a wider range, event-level information affects verbal arguments processing of human \citep{bicknell2010effects}. Comprehenders integrate information not only from predicate but also other event participants mentioned in discourse. Although there are plenty of results from different forms of psycholinguistic experiments like event-related potential, eye-tracking and human judgement, most of these experiments have a relatively small size of evaluation entries and a limited number of participants. It has been shown that corpus-based method can be considered as a large-scale simulation of human acquisition of linguistic or conceptual information from the world \citep{landauer1997solution}. To verify the results from psycholinguistic experiments, it is necessary to build corpus-based cognitive models of events. 

\subsection{Event Representation and Semantic Role Model} \label{sec:semanticrole}
In early research, events were represented using first order logic. For the event in the example sentence (\ref{eg:sent1}), the representation is:
\begin{equation*} \label{eg:fol}
\begin{aligned}
    & cut(\text{cook}, \text{cake}) \land with(\text{knife}) \land in(\text{kitchen}) \\
\end{aligned}
\end{equation*}
where it is hard to refer to predicates. Later on, \citet{davidson1967logical} proposed a method that linguistic predicates consider an event as one of their logical arguments explicitly. So the representation of the sentence (\ref{eg:sent1}) becomes: 
\begin{equation*} \label{eg:davidsonian}
\begin{aligned}
    & \exists e\ cut(e, \text{cook}, \text{cake}) \land with(e, \text{knife}) \land in(e, \text{kitchen}) \\
\end{aligned}
\end{equation*}
where the predicate $cut$ is a description of an event $e$. This representation, named as \textit{Davidsonian event representation}, has a drawback that it is hard to distinguish the roles of $cook$ and $cake$. Combining with thematic roles mentioned in session \ref{sec:intro}, \citet{parsons1990events} updated the representation to: 
\begin{equation} \label{eg:neodavidsonian}
\begin{aligned}
    \exists e\ cut(e)
    & \land \texttt{AGENT}(e, \text{cook}) \land \texttt{PATIENT}(e, \text{cake}) \\
    & \land \texttt{INSTRUMENT}(e, \text{knife}) \land \texttt{LOCATION}(e, \text{kitchen}) \\
\end{aligned}
\end{equation}
where each thematic role is acting as a logical predicate of the event $e$ and the corresponding participant. This representation, named as \textit{neo-Davidsonian event representation}, is highly compatible with semantic role models. 
% * <asayeed@mbl.ca> 2017-11-13T22:37:07.932Z:
% 
% Unfortunately neo-Davidsonian representations and the psycholinguistic work on this topic are only loosely connected. We introduced neo-Davidsonian representations to allow for incrementality in PLTAG-based semantic parsing.  Otherwise, most psycholinguistic work doesn't care about the formal status of the event.  Unless you want to justify things in terms of the output representation of a parse, is this necessary?
% 
% ^.

Thematic role is the oldest semantic role model which is formalised by \citet{gruber1965studies} and \citet{fillmore1968case} in modern time. The inventory of roles usually contains \texttt{AGENT}, \texttt{PATIENT}, \texttt{INSTRUMENT} and \texttt{LOCATION} \citep{aarts2013english}. Table \ref{tab:thematic} shows informal definitions of them \citep{jurafsky2014speech}. One disadvantage of thematic roles is that the range of role inventory is difficult to decide. Even worse, the formal definitions of thematic roles are controversial. 

The Proposition Bank (PropBank) is an annotated corpus of semantic roles based on Penn TreeBank \citep{palmer2005proposition}. It also provides an effective semantic role model which we will adopt. Table \ref{tab:propbank} shows examples of semantic roles in PropBank style. The definitions are based on the PropBank Annotation Guidelines \citep{bonial2010propbank}. Arguments required for the valency of a predicate are listed with numbers. Modifiers, which are relatively stable across predicates, are marked with ArgMs. While PropBank focuses on verbal predicates, NomBank adds annotations to nominal predicates \citep{meyers2004nombank}. 


\begin{table}[t]
\centering
\begin{tabular}{l|l} 
\hline
\textbf{PropBank Semantic Role}  &   \textbf{Definition} \\ \hline
\texttt{Arg0}                    &   proto-agent, the argument that most likely to be the agent, \\ 
                                 &   volitional causer or experiencer \\ \hline
\texttt{Arg1}                    &   proto-patient, the argument that most likely to be the patient \\ \hline
\texttt{Arg2}                    &   the instrument, benefactive or attribute \\ \hline
\texttt{Arg3}                    &   the starting point, benefactive or attribute \\ \hline
\texttt{Arg4}                    &   the ending point \\ \hline
\texttt{ArgM-LOC}                &   the argument that indicates where the action takes place \\ \hline
\texttt{ArgM-MNR}                &   the argument that specifies how the action is performed \\ \hline
\texttt{ArgM-TMP}                &   the argument that shows when an action took place \\ \hline
\texttt{ArgM-PNC}                &   the argument that shows motivation for some action \\ \hline
\texttt{ArgM-CAU}                &   the argument that indicates the reason for an action \\ \hline
\end{tabular}
\caption{\label{tab:propbank} Examples of semantic roles in PropBank style.}
\end{table}
% * <asayeed@mbl.ca> 2017-11-13T22:39:14.487Z:
% 
% There are even more, but we don't use them.  And we don't use ARGM-CAU, do we?
% 
% ^ <xhong@coli.uni-saarland.de> 2017-11-16T13:00:01.637Z:
% 
% This is a summary of PropBank role. The role labels we use are in section 3. 
% 
% ^ <asayeed@mbl.ca> 2017-11-16T21:54:38.832Z:
% 
% OK make it clear that this is only a sample of the ARGM ones.
%
% ^ <xhong@coli.uni-saarland.de> 2017-11-17T15:40:07.201Z.


\subsection{Semantic Role Labelling} \label{sec:srl}
The \textit{semantic role labelling} (SRL) task is to assign semantic role labels to arguments of predicates in sentences automatically. It is a well-study task which was defined formally at CoNLL 2004, 2005 shared tasks which aimed to annotate phrasal arguments of verbal predicates in the sentences \citep{carreras-marquez:2004:CONLL, carreras-marquez:2005:CoNLL}. The CoNLL 2008, 2009 shared tasks introduced a new task where the semantic dependencies are annotated instead of phrasal arguments \citep{surdeanu-EtAl:2008:CONLL, hajivc-EtAl:2009:CoNLL-2009-ST}. 

After many years of research, pre-trained semantic role labelling systems are quite reliable. Generally speaking, modern SRL system is a statistical model that can predict the semantic role labels given the sentences. Many early work used syntactic parses as input and estimate classifiers locally with a set of constraints \citep{punyakanok2008importance}. \citet{tackstrom2015efficient} interpreted the model as a graphical model and use a multi-layer neural network to enforce the constraints. Other works focus on the application of distributed representation to avoid feature engineering and result in better generalisation. \citet{collobert2007fast, collobert2011natural} proposed a convolution neural network model using a distributed representation that only relies on lexical features. The fine-tuned model with additional features, SENNA, has competitive performance. The running time of SENNA is fast so it can be used on a large corpus. 
% * <asayeed@mbl.ca> 2017-11-13T22:40:52.091Z:
% 
% SENNA isn't state of the art performance anymore, is it? And after citing Täckström 2015, you can't say that 2007 and 2011 papers are "later works", now can you?
% 
% ^ <xhong@coli.uni-saarland.de> 2017-11-16T13:03:41.920Z:
% 
% Changed to 'Other works'
%
% ^.


More recently, \citet{lei-EtAl:2015:NAACL-HLT} employ low-rank tensor factorisation to induce a compact representation of the full cross product of atomic features. \citet{roth2016neural} proposed PathLSTM which made use of dependency path as input. \citet{marcheggiani2017simple} claimed that syntactic features are not necessary for semantic role labelling and proposed a deep LSTM model to obtain competitive performance. 

However, most of these works only assign labels to whole constituents instead of syntactic heads that are denoting event participants. \citet{SayeedEtAl2015} proposed an unsupervised method of assigning semantic roles to syntactic heads in arguments. The semantic role labels are extracted using SENNA. Then the head nouns are extracted from arguments using Malt Parser, a state-of-the-art dependency parser \citep{nivre2006maltparser}, together with hand-crafted heuristics. 


\subsection{Distributional Semantic Model} \label{sec:dsm}
Distributional semantic model (DSM) is a corpus-based method to construct numerical semantic representation. DSM depends on \textit{distributional hypothesis} \citep{harris1954distributional, miller1991contextual} claiming that the semantic similarity between two linguistic units can be measured by the overlap among their contexts. A DSM can be viewed as an order-2 tensor where one dimension is indices of words (or lexical units) and another dimension is features that used to capture context information. Generally, in early work of DSM, a matrix was constructed based on lexical co-occurrences of words in a large corpus. Then each word can be represented by a high-dimensional sparse vector in this matrix. The semantic similarity between two units can be quantified as the similarity of two representation vectors. This, however, does not consider the linguistic information within the context. To include such information, later work encode syntactic relation or lexico-syntactic pattern \citep{pado2007integration, erk2008structured, rothenhausler2009unsupervised} (TODO: performance?). 

\citet{baroni2010distributional} claimed that it is possible to construct a general model for semantic memory, a stable long-term knowledge of human, to tackle different semantic tasks. They proposed \textit{Distributional Memory} (DM) framework which is an order-3 tensor which contains word-relation-word tuple. Tensor space is constructed on a count space of those tuples from large-scale corpus. The weights in the tensor space is smoothed by local mutual information (LMI). Performing matricisation on the tensor onto a matrix, vectors from different semantic spaces can be obtained. The model is constructed on a mixed corpus, named \textbf{UBW} in this paper, containing ukWac  \citep{ferraresi2008introducing}, BNC \citep{british2007british} and Wikipedia. On tasks of a broad range, their best model, named as TypeDM, obtained competitive performance. This can be interpreted as the advantage of the distributional representation on multiple semantic tasks. Furthermore, the tensor like structure is both flexible and expressive for semantic information modelling. 

In addition to syntactic information, \citet{sayeed2014combining} extracts shallow semantic representation using semantic role labeller SENNA and constructs DM upon them. Combining syntactic and semantic features, they build a fully unsupervised distributional memory model, named as SDDM, especially for thematic fit judgement task. SDDM outperforms syntax-based thematic fit model by a large margin, which indicates that semantic information is essential in the thematic fit task. To improve the performance of DM on thematic fit task, \citet{greenberg2015improving} employed clustering on candidate role-fillers to construct a better representation for prototypical role-filler. \citet{santus2017measuring} proposed a feature selection method which extract the most significant \textit{second order contexts} for each role of each verb and compute thematic fit score as a weighted overlap between the top features of role-fillers and prototype role-filler. 

There are three major drawbacks of the distributional semantic model. Firstly when it comes to practice, the semantic space of DSM is high-dimensional and sparse. When someone wants to induce on complex events with a large number of participants, the size of representation becomes enormous. It is time or space consuming to use this kind of representation in practice. In addition, the parameters of the vectors are discrete in count-based vector space. Although many alternative weighting functions and smoothing techniques have been proposed, it is hard to decide which is the most optimised. Last but not least, these models usually suffer the curse of dimensionality. Because the Euclidean distance between two vectors is meaningless in high-dimensional space, when the representation vectors are used as input of distance-based algorithms like nearest-neighbour classifier or clustering, the result can be unexpected. Thus we need a better representation model in this paper. 
% * <asayeed@mbl.ca> 2017-11-13T22:43:00.518Z:
% 
% Why is using sparse vectors infeasible for complex events? Because of the multiplicative nature of event composition in e.g. the Lenci 2011 (?) model of compositionality via DMs.
% 
% ^ <xhong@coli.uni-saarland.de> 2017-11-16T13:10:12.720Z:
% 
% I added more details. It is changed to ' the size of representation becomes enormous. It is time or space consuming to use this kind of representation in practice. '
% 
% ^.


\subsection{Distributed Representation} \label{sec:repr}
Distributed representation, pioneered by \citet{hinton1986learning}, is an effective method to handle sparsity and curse of dimensionality in symbolic representation. In general symbolic representation like \textit{one-hot representation}, where one binary value is $1$ and the others are $0$ in representation vectors, the binary values are mutual exclusive. If each representation vector has $n$ features, it can only represent $n$ different states of the entry. In distributed representation, $n$ feature with $k$ possible values can describe $k^n$ different states \citep{Goodfellow-et-al-2016}. This is why distributed representation is powerful. 

The distributed representation of words, popularly known as word embedding, can be learnt in the training of \textit{neural net language model} \citep{bengio2003neural}. Most of the early works aimed to solve language modelling which is the task to predict the target word given its context. Unlike DSMs or other count-based semantic models, the parameters in distributed representation vectors are learnt during the training of neural net language models. Later these models were further improved by making use of sequential information in the previous context of target words. This can be done by adding recurrent units to hidden layers of neural net language model \citep{mikolov2010recurrent}. To learn high quality word embeddings with million words in the vocabulary from a billion scale of text, \citet{mikolov2013efficient} proposed two models, \textit{CBOW} and \textit{Skip-gram}, based on logistic regression. Context is the words around the target word in both models. In CBOW model, context representation is the sum of word embedding vectors of the words in the context. In Skip-gram model, the target word is used to predict the words in the context. The system they built, famously known as \textit{word2vec}, obtained competitive performance against more complex neural networks on syntactic and semantic relation tasks. 



 
\section{Representation of Predicates and Participants}
We intend to learn meaningful representation of event structure in semantic space. Thus we define a semantic role model for this paper. We join PropBank semantic roles and the special role \texttt{PRD} together as semantic roles. So the semantic role labels we use in this paper are defined in Table \ref{tab:semantic}. As we mention in Section \ref{sec:intro}, semantic arguments can be represented by the syntactic head words, so the notion of \textit{argument} in PropBank role labels is replaced by \textit{event participant}. 
% * <asayeed@mbl.ca> 2017-11-13T22:45:29.318Z:
% 
% Again do we need to commit to this kind of representation here?  I'm not saying categorically not to do it, merely that it needs to be justified.
% 
% ^.

\begin{table}[t]
\centering
\begin{tabular}{l|l|l} 
\hline
\textbf{ID} &   \textbf{Semantic Role Label}    &   \textbf{Definition} \\  \hline
0           & \texttt{PRD}       &   the predicate denoting the event \\ \hline
1           & \texttt{ARG0}      &   \textbf{proto-agent}, the event participant that most likely to be the agent, \\     &&   volitional causer or experiencer \\ \hline
2           & \texttt{ARG1}      &   \textbf{proto-patient}, the event participant that most likely to be the patient, \\ &&   the event participant which is being affected by the action \\ \hline
3           & \texttt{ARG2}      &   the event participant which is secondly likely to be the patient,  \\       &&   the benefactive or attribute \\ \hline
4           & \texttt{ARGM-MNR}  &   the event participant that specifies how the action is performed \\ \hline
5           & \texttt{ARGM-LOC}  &   the event participant that indicates where the action takes place \\ \hline
6           & \texttt{ARGM-TMP}  &   the event participant that shows when an action took place \\ \hline
\end{tabular}
\caption{\label{tab:semantic} Definitions of semantic role labels in this paper.}
\end{table}


\subsection{Event-Participant Embedding} \label{sec:epe}
After obtaining the semantic role model, we define the symbolic representation for words. Word $a_i$ with identifier $i$ is encoded as one-hot row vector $\mathbf{w}_i$ where $|\mathbf{w}_i| = |V|$ and $V$ is the word vocabulary. In this vector space, words are mutual exclusive to each other. Similarly, we can represent semantic role label $b_j$ with identifier $j$ using one-hot encoded column vector $\mathbf{r}_j$ where $|\mathbf{r}_j| = |R|$ and $R$ is the set of semantic role labels. 

Further, we need to obtain a distributed representation that is continuous in the feature space. So we compute the inner product between $\mathbf{w}_i$ and a embedding matrix $\mathbf{A} \in \mathbb{R}^{|V| \times d}$ as $\mathbf{A}_{(i)} = \mathbf{w}_i \mathbf{A}$. The vector $\mathbf{w}_i$ is now mapped to low-dimensional vector $\mathbf{A}_{(i)} \in \mathbb{R}^d$ which is the $i$-th row of matrix $\mathbf{A}$. The symbolic representation of words is transformed into word embedding. 

In addition to word embedding, we want to learn the representation of words given a specific semantic role label. We add one more dimension of semantic role label to the word embedding matrix and obtain a third-order tensor $\mathbf{T} \in \mathbb{R}^{|V| \times |R| \times d}$. For each word $\mathbf{w}_i$ and each semantic role label $\mathbf{r}_j$, there is a corresponding vector $\mathbf{T}_{(ij)}$ with length of $d$ where $i \in [1, |V|]$ and $j \in [1, |R|]$. We define this vector as \textbf{event-participant embedding vector}. 


\subsection{Tensor Factorisation} \label{sec:tf}
There are $|V| \times |R| \times d$ parameters in the order-3 tensor $\mathbf{T}$. Since word vocabulary size $|V|$ is often very big (more than $30,000$), the number of parameters in the tensor grows tremendously when $|R|$ or $d$ increases, which makes the model hard to converge. The huge number of parameters in event-participant embedding can result in curse of dimensionality. Moreover, \citep{tilk2016event} pointed out that this kind of embedding tensor lacks parameter sharing across word embedding matrices between different role labels, while words with the same semantic role label actually have common characteristics. In order to capture these characteristics, we need a method to share weights between word embedding matrices. 

We perform tensor rank decomposition \citep{hitchcock1927expression} on the embedding tensor in real space:
\begin{equation} \label{eq:trd-org}
\begin{aligned}
    \mathbf{T}   
        &= \sum_{m=1}^{k} \lambda_m \mathbf{T}'_m \\
        &= \sum_{m=1}^{k} \lambda_m \mathbf{a}_m \otimes \mathbf{b}_m \otimes \mathbf{d}_m, \\
\end{aligned}
\end{equation}
where tensor $\mathbf{T}$ is decomposed into $k$ rank-1 tensor $\mathbf{T}'$ and $\lambda_m$ is the weight of rank $m$. Each rank-1 tensor can be written as outer product of 3 row vectors $\mathbf{a}_m$, $\mathbf{b}_m$ and $\mathbf{d}_m$. Since all parameters are learnt during training, we let $\mathbf{c}_m$ equal to $\lambda_m \mathbf{d}_m$. So the equation becomes:
\begin{equation} \label{eq:trd}
\begin{aligned}
    \mathbf{T}   
        &= \sum_{m=1}^{k} \mathbf{a}_m \otimes \mathbf{b}_m \otimes \mathbf{c}_m, \\
\end{aligned}
\end{equation}
where the smallest $k$ makes the equation true is the tensor rank of $\mathbf{T}$. Now the number of parameters is reduced to $(|V| + |R| + d) \times k$ which is comparable smaller. 

\subsection{Interpretation of Factored Tensor} \label{sec:tf-expl}
To get a better matrix representation of the factored tensor in equation \eqref{eq:trd}, we group $k$ vectors $\mathbf{a}_m$, $\mathbf{b}_m$ by column into matrices $\mathbf{A} \in \mathbb{R}^{|V| \times k}$, $\mathbf{B} \in \mathbb{R}^{|R| \times k}$ and $\mathbf{c}_m$ by row into matrices $\mathbf{C} \in \mathbb{R}^{k \times d}$. As a result, word $i$ can be represented by a matrix: 
\begin{equation} \label{eq:we-tensor}
\begin{aligned}
    \mathbf{T}_{(i)}
        &= \sum_{m=1}^{k} \mathbf{a}_{m_{(i)}} \otimes \mathbf{b}_{m} \otimes \mathbf{c}_m \\
        &= \mathbf{B} \ diag(\mathbf{w_i}\mathbf{A}) \ \mathbf{C}
\end{aligned}
\end{equation}
where $diag(\mathbf{v})$ is a diagonal matrix with vector $\mathbf{v}$ on its diagonal. Although there is an intuitive mapping between matrix $A$ and word embedding matrix we defined above, they are not equal to each other. Thus we claim that word embedding matrix $\mathbf{T}_{(i)}$ is the result of word representation vector $\mathbf{w_i}\mathbf{A}$ after several linear mappings. 

Due to the fact that in a vector space, the dot product of the matrix $diag(\mathbf{w_i}\mathbf{A})$ with either $\mathbf{B}$ or $\mathbf{C}$ can be considered as performing a linear mapping. So we can consider $diag(\mathbf{w_i}\mathbf{A})$ as the embedding of word $i$. Focusing on the non-zero part, the embedding becomes the vector $\mathbf{w_i}\mathbf{A}$ on the diagonal. Naturally, the matrix $\mathbf{A}$ can be interpreted as word embedding matrix described above. A similar interpretation can also be applied to semantic role label embedding. Each semantic role label can be represented by a matrix:
\begin{equation} \label{eq:re-tensor}
\begin{aligned}
    \mathbf{T}_{(j)}
        &= \sum_{m=1}^{k} \mathbf{a}_{m} \otimes \mathbf{b}_{m_{(j)}} \otimes \mathbf{c}_m \\
        &= \mathbf{A} \ diag(\mathbf{r_j}\mathbf{B}) \ \mathbf{C}
\end{aligned}
\end{equation}
By computing inner product between $\mathbf{r}_j$ and $\mathbf{B}$ as $\mathbf{B}_{(j)} = \mathbf{r}_j \mathbf{B}$, we obtain the low-dimensional vector $\mathbf{B}_{(j)} \in \mathbb{R}^k$ which is the $j$-th row of matrix $\mathbf{B}$. $\mathbf{B}$ can be considered as role embedding matrix. From equation \eqref{eq:trd}, the event-participant embedding vector $\mathbf{T}_{(ij)}$ can be computed as: 
\begin{equation} \label{eq:rbwe-tensor}
\begin{aligned}
    \mathbf{T}_{(ij)}
        &= \sum_{m=1}^{k} \mathbf{a}_{m_{(i)}} \otimes \mathbf{b}_{m_{(j)}} \otimes \mathbf{c}_m \\
        &= \sum_{m=1}^{k} \mathbf{a}_{m_{(i)}} \mathbf{b}_{m_{(j)}}  \mathbf{c}_m \\
\end{aligned}
\end{equation}

To get rid of the outer product and make it easier to implement, we use the decomposition matrices defined above. Thus the vector $\mathbf{T}_{(ij)}$ can be written as a vector: 
\begin{equation} \label{eq:rbwe}
\begin{aligned}
    \mathbf{p} 
        &= (\mathbf{A}_{(i)}\circ \mathbf{B}_{(j)}) \mathbf{C} \\
        &= (\mathbf{w}_i \mathbf{A} \circ \mathbf{r}_j \mathbf{B}) \mathbf{C} \\
\end{aligned}
\end{equation}
which is a representation of a participant in an event. "$\circ$" is Hadamard product which computes element-wise multiplication between two vectors. This can be interpreted as a multiplicative composition method of word embedding and role embedding. 



 
\section{Corpus and Prepossessing} \label{sec:corpus}
In order to learn event-participant embedding from language resources, we use Rollenwechsel-English (\textbf{RW-eng}) corpus, a large-scale labelled semantic corpus with a head word of each argument using PropBank style role labels \citep{SayeedEtAl2015}. With head words of arguments, we have a direct access to the event participants without modifiers. This helps us to learn embeddings of generalised event knowledge. 

The corpus, extracted from the ukWaC corpus and BNC, contains about 78 million sentences over 2.3 million documents. Among those sentences, there are approximately 210 million event predicates and 710 million event participants. They divided the corpus into 3500 segments with approximately same number of documents. The sentences are labelled using SENNA, a syntactic agnostic semantic role labeller with competitive performance, where sentences with multiple predicates are separated into several events. The head words are extracted with one of four heuristics: \texttt{MALT}, \texttt{MALT-SPAN}, \texttt{LINEAR} or \texttt{FAILED}. One sample event in the corpus is listed here: 
\definecolor{bg}{rgb}{0.95, 0.95, 0.95}
\begin{minted}
[frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=bg,
fontsize=\footnotesize,
linenos]{xml}
<text id="ukwac:http://www.exeterviews.co.uk/exeter-shopping2.php" />
<s>
<predicate>
  <governor>release/vbg/13</governor>
  <dependencies>
    <dep algorithm="MALT" source="the/DT/3 large/JJ/4
    stone/NNS/5" text="the large stone"
    type="A0">stone/nns/5</dep>
    <dep algorithm="FAILED" source="which/WDT/6" text="which"
    type="R-A0">which/wdt/6</dep>
    <dep source="release/VBG/13"
    text="release" type="V">release/vbg/13</dep>
    <dep algorithm="MALT_SPAN" source="their/PRP$/14 heat/NN/15"
    text="their heat" type="A1">heat/nn/15</dep>
    <dep algorithm="LINEAR" source="at/IN/16 night/NN/17"
    text="at night" type="AM-TMP">night/nn/17</dep>
    <dep algorithm="MALT" source="to/TO/18 ensure/VB/19 the/DT/20
    grape/NNS/21 achieve/VBP/22 maximum/JJ/23 ripeness/NN/24"
    text="to ensure the grape achieve maximum ripeness"
    type="AM-PNC">achieve/vbp/22</dep>
  </dependencies>
</predicate>
\end{minted}
where event is arranged in XML format. Line $1$ contains the URL to the text containing this event. The predicate is tagged as $governor$ and the arguments are tagged as $dep$. The head word is in the value field of $dep$ and the heuristic used to extract it is in the attribute $algorithm$. 

We choose the first 3472 segments as training corpus, the next 4 segments as test corpus, the next 4 segments as validation corpus, and keep the remaining segments for further experiments. The word list for event predicates and participants is constructed with following steps: 
\begin{enumerate}
  \item  go through the training corpus, for each event, find out the entries denoting the event predicate and all event participants. 
  \item  filter out every entry that has \texttt{FAILED} in $algorithm$; only accept entry with POS tag starting with $n$, $v$, $j$, $r$; only accept entry with English characters and the dash line $-$; 
  \item  extract the lower case form of the head word from each entry.
  \item  lemmatise each word using WordNet \citep{miller1995wordnet} lemmatiser in NLTK \citep{bird2006nltk}. 
  \item  compute the frequency of each unique lemma. 
  \item  generate the vocabulary from the top $50000$ most frequent words. 
\end{enumerate}
Using this word list, each word $a_i$ is assigned with a identifier $i$ in the integer set of $[1, 50000]$. The identifier of out-of-vocabulary word \texttt{<OOV>} is $50001$. So the length of the word vocabulary $V$ is 50001. The role vocabulary is constructed upon the list of semantic role labels defined in Table \ref{tab:semantic} with an additional role label \texttt{<OTHER>} for those role labels in the corpus but not in our list. Since we have $8$ role labels in the role vocabulary $R$, each role $b_j$ has a identifier $j$ in the range of $[0, 7]$ where the identifier $7$ is for the \texttt{<OTHER>} role. 

Using these identifiers, we go through all three corpora to construct training data, test data and validation data. Similar to vocabulary construction, we filter out all event with \texttt{FAILED} in $algorithm$. In each event, the maximum number of roles is $8$. If there are more than one \texttt{<OTHER>} role in the event, we accept the last one. For each role represented by its identifier, it is coupled with a word identifier for the corresponding head word. For any role without a head word, an identifier $0$ for empty word \texttt{<NULL>} is assigned. The line of the sample event in the data is listed here: 
\begin{equation} \label{eq:eg-data}
\begin{aligned}
    \{ 0:9527,\ 1:20,\ 2:50,\ 3:0,\ 4:0,\ 5:0,\ 6:3123,\ 7:4311\}
\end{aligned}
\end{equation}


 
\section{Multi-task Event-Participant Model} \label{sec:mtep}


\begin{figure}[t]
\centering
\includegraphics[width=0.65\textwidth]{NNRF.png}
\caption{\label{fig:NNRF} Model architecture of non-incremental role filler.}
\end{figure}


\subsection{Non-incremental Role-Filler Model} \label{sec:nnrf}
To perform the role-filler prediction, \citet{tilk2016event} proposed the \textit{non-incremental role-filler} \textbf{(NNRF)} which is a 2-layer neural network model. As in Figure \ref{fig:NNRF}, from input to output, the model uses event-participant embeddings as input. Parameters are learnt together with the model. The embedding vector of the $l$-th entry in an event can be represented as:
\begin{equation} \label{eq:rbe-nnrf}
\begin{aligned}
    \mathbf{p}_l
        &= (\mathbf{w}_i \mathbf{A}_e \circ \mathbf{r}_j \mathbf{B}_e) \mathbf{C}_e \\
\end{aligned}
\end{equation}
The embedding vectors are composed using sum method into an representation of words except the target word as:
\begin{equation} \label{eq:sum-comp}
\begin{aligned}
    \mathbf{e}
        &= \sum_{l=1}^{L} \mathbf{p}_{l} \\
\end{aligned}
\end{equation}
where $l$ is the index of the word and $L$ is the number of words in current event $e$. Then it passes through one non-linearity layer with parametric rectified linear unit \citep{he2015delving} as follow:
\begin{equation} \label{eq:nonlinearity-nnrf}
\begin{aligned}
    \mathbf{h}
        &= PReLU(\mathbf{e} + \mathbf{b}_e) \\
\end{aligned}
\end{equation}
where $\mathbf{b}_e$ is the bias vector of this layer. After that, a softmax regression classifier is used for prediction. The output vector of this classifier is computed as:
\begin{equation} \label{eq:output-nnrf}
\begin{aligned}
    \mathbf{o}
        &= \mathbf{h}\mathbf{W}_c + \mathbf{b}_c \\
\end{aligned}
\end{equation}
where $\mathbf{W}_c\in \mathbb{R}^{d \times |V|}$ is a weight matrix of the target word classifier and $\mathbf{b}_c$ is the corresponding bias. The conditional probability of target word $a_x$ given event $e$ and target role $b_y$ is:
\begin{equation} \label{eq:softmax-nnrf}
\begin{aligned}
    p(a_x | e, b_y)
        &= Softmax(\mathbf{o})_{(x)} \\
        &= \frac{
        exp(\mathbf{o})_{(x)}
        }{
        \sum_{i=1}^{|V|} exp(\mathbf{o})_{(i)} }   \\
\end{aligned}
\end{equation}
where suffix $(x)$ is the $x$-th element of the vector. 

For each target target role, there is a different weight matrix to perform classification. \citet{tilk2016event} applied a factorisation for the classifiers, which is similar to the method for role-specific word embedding. At first weight matrix $\mathbf{W}_c^{(b_y)}$ is defined for each target role $b_y$. Then those matrices $\mathbf{W}_c^{(b_y)}$ are stacked from $b_1$ to $b_{|R|}$. A weight tensor $\mathbf{T}^{(c)} \in \mathbb{R}^{d \times |R| \times |V|}$ is obtained. But this tensor suffer the same dimensionality problem as event-participant embedding tensor. So tensor rank decomposition is applied again on the weight tensor, and extract the corresponding weight matrices for $b_y$ as follow: 
\begin{equation} \label{eq:trd-cls}
\begin{aligned}
    \mathbf{T}_{(y)}^{(c)}
        &= \sum_{m=1}^{k^{(c)}} \mathbf{c}_{m}^{(c)} \otimes \mathbf{b}_{m_{(y)}}^{(c)} \otimes \mathbf{a}_m^{(c)} \\
\end{aligned}
\end{equation}
Similarly, vectors $\mathbf{c}_{m}^{(c)}$ are grouped by row into matrices $\mathbf{C}_c \in \mathbb{R}^{d \times k^{(c)}}$ and $\mathbf{a}_m^{(c)}$ are grouped by column into matrices $\mathbf{A}_c \in \mathbb{R}^{k^{(c)} \times |V|}$. After getting rid of outer product using same transformation as equation \eqref{eq:rbwe}, the weight matrix in equation \eqref{eq:output-nnrf}  can be written as:
\begin{equation} \label{eq:cls}
\begin{aligned}
    \mathbf{W}_c
        &= \mathbf{C}_c \ diag(\mathbf{b}_y) \ \mathbf{A}_c \\
\end{aligned}
\end{equation}
Since there is a vector $\mathbf{b}_y$ representing target role $b_y$, the embedding matrices $\mathbf{B}_c \in \mathbb{R}^{k^{(c)} \times |R|}$ can be defined for target roles. The weight matrices can be further written as:
\begin{equation} \label{eq:cls-temb}
\begin{aligned}
    \mathbf{W}_c
        &= \mathbf{C}_c \ diag(\mathbf{r}_y \mathbf{B}_c) \ \mathbf{A}_c \\
\end{aligned}
\end{equation}
Using the semantic roles we define in Table \ref{tab:semantic}, the NNRF model can perform both predicate prediction and participant prediction. 


\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{MTRF.png}
\caption{\label{fig:MTEP} Model architecture of multi-task version of non-incremental role filler.}
\end{figure}


\subsection{Multi-task Learning Model} \label{sec:mtl}
We also want to perform semantic role prediction together with predicate prediction and participant prediction. Multi-task learning can improve generalisation of the model \citep{caruana1998multitask}. In natural language processing, \citet{collobert2011natural} proposed a model which shares the word embedding across tasks of language modelling, part-of-speech tagging, chunking, named entity recognition, semantic role labelling. They reported improvement on each task. 

The semantic role prediction task can be considered as a regularisation of the objective functions of the other tasks. We add one more classifier for this task to NNRF in the last layer. The model, named \textbf{multi-task event-participant (MTEP)} model, is described in Figure \ref{fig:MTEP} where the extra classifier is in the red border rectangle. 

Since we want to obtain the probability distribution of target word or target role, in the last layer a softmax regression classifier is used for each task. The output vectors of these classifiers are computed as:
\begin{equation} \label{eq:output-mt}
\begin{aligned}
    \mathbf{o}_w
        &= \mathbf{e}\mathbf{W}_w + \mathbf{b}_w \\
    \mathbf{o}_r
        &= \mathbf{e}\mathbf{W}_r + \mathbf{b}_r \\
\end{aligned}
\end{equation}
where $\mathbf{W}_w \in \mathbb{R}^{d \times |V|}$ is a weight matrix of the target word classifier,  $\mathbf{W}_r \in \mathbb{R}^{d \times |R|}$ is a weight matrix of the target role classifier and $\mathbf{b}_w$, $\mathbf{b}_r$ are the corresponding biases of the classifier. The conditional probability of target word $a_x$ given event $e$ and target role $b_y$ is:
\begin{equation} \label{eq:softmax-w}
\begin{aligned}
    p(a_x | e, b_y)
        &= Softmax(\mathbf{o}_w)_{(x)} \\
        &= \frac{
        exp(\mathbf{o}_w)_{(x)}
        }{
        \sum_{i=1}^{|V|} exp(\mathbf{o}_w)_{(i)} }   \\
\end{aligned}
\end{equation}
where suffix $(x)$ is the $x$-th element of the vector. And the conditional probability of target role $b_y$ given event $e$ and target word $a_x$ is:
\begin{equation} \label{eq:softmax-r}
\begin{aligned}
    p(b_y | e, a_x)
        &= Softmax(\mathbf{o}_r)_{(y)} \\
        &= \frac{
        exp(\mathbf{o}_r)_{(y)}
        }{
        \sum_{j=1}^{|R|} exp(\mathbf{o}_r)_{(j)} }   \\
\end{aligned}
\end{equation}

For each target role or target word, we apply tensor factorisation again to obtain target role/word specific classifiers. At first we define weight matrix $\mathbf{W}_w$ and weight matrix $\mathbf{W}_r$ for each target word $a_x$ and target role $b_y$. Then we stack $\mathbf{W}_w$ and $\mathbf{W}_r$ from $a_1$ to $a_{|V|}$ and from $b_1$ to $b_{|R|}$. Two weight tensors $\mathbf{T}^{(w)} \in \mathbb{R}^{d \times |R| \times |V|}$ and $\mathbf{T}^{(r)} \in \mathbb{R}^{d \times |V| \times |R|}$ are obtained. But these tensors suffer the same dimensionality problem as event-participant embedding tensor. So we apply tensor rank decomposition again on two weight tensors, and extract the corresponding weight matrices for $a_x$ and $b_y$ as follow: 
\begin{equation} \label{eq:trd-mt-cls}
\begin{aligned}
    \mathbf{T}_{(x)}^{(w)}
        &= \sum_{m=1}^{k^{(w)}} \mathbf{c}_{m}^{(w)} \otimes \mathbf{b}_{m_{(x)}}^{(w)} \otimes \mathbf{a}_m^{(w)} \\
    \mathbf{T}_{(y)}^{(r)}
        &= \sum_{m=1}^{k^{(r)}} \mathbf{c}_{m}^{(r)} \otimes  \mathbf{a}_{m_{(y)}}^{(r)} \otimes \mathbf{b}_m^{(r)} \\
\end{aligned}
\end{equation}
Similarly, we group vectors $\mathbf{c}_{m}^{(w)}$, $\mathbf{c}_{m}^{(r)}$ by row into matrices $\mathbf{C}_w \in \mathbb{R}^{d \times k^{(w)}}$, $\mathbf{C}_r \in \mathbb{R}^{d \times k^{(r)}}$ and group $\mathbf{a}_m^{(w)}$, $\mathbf{b}_m^{(r)}$ by column into matrices $\mathbf{A}_w \in \mathbb{R}^{k^{(w)} \times |V|}$, $\mathbf{B}_r \in \mathbb{R}^{k^{(r)} \times |R|}$. After getting rid of outer product using same transformation as equation \eqref{eq:rbwe}, two weight matrices in equation \eqref{eq:output-nnrf}  can be written as: 
\begin{equation} \label{eq:cls-mt}
\begin{aligned}
    \mathbf{W}_w
        &= \mathbf{C}_w \ diag(\mathbf{b}_y) \ \mathbf{A}_w \\
    \mathbf{W}_r
        &= \mathbf{C}_r \ diag(\mathbf{a}_x) \ \mathbf{B}_r \\
\end{aligned}
\end{equation}
where $diag(\mathbf{v})$ is a diagonal matrix with vector $\mathbf{v}$ on its diagonal. Since there is a vector $\mathbf{b}_y$ representing target role $b_y$ and there is a vector $\mathbf{a}_x$ representing target word $a_x$. Now we can define the embedding matrices $\mathbf{B}_w \in \mathbb{R}^{k^{(w)} \times |R|}$, $\mathbf{A}_r \in \mathbb{R}^{k^{(r)} \times |V|}$ for target roles and target words. The weight matrices can be further written as: 
\begin{equation} \label{eq:cls-mt-temb}
\begin{aligned}
    \mathbf{W}_w
        &= \mathbf{C}_w \ diag(\mathbf{r}_y \mathbf{B}_w) \ \mathbf{A}_w \\
    \mathbf{W}_r
        &= \mathbf{C}_r \ diag(\mathbf{w}_x \mathbf{A}_r) \ \mathbf{B}_r \\
\end{aligned}
\end{equation}


\subsection{Model Training} \label{sec:training}
The MTEP model is trained to optimise three objective functions in a multi-task style. For each event in the training data, we define the likelihood functions for the three tasks defined in Section \ref{sec:tasks}:
\begin{itemize}
  \item  \textbf{Participant prediction}: We choose the $l$-th head word which is not empty word. We use the head word as target word $a_l$ and its semantic role as target role $b_l$. Then we consider all other word-role pairs as event context $e$. The likelihood function is computed using output from Equation \eqref{eq:softmax-w} as:
  \begin{equation} \label{eq:likelihood-pa}
  \begin{aligned}
  \mathcal{L}^{(pa)}(\theta, e) = \prod_{i = 1}^{L-1} p(a_l | e, b_l)
  \end{aligned}
  \end{equation}
where $L-1$ is the number of event participants in the event (where the predicate is not included). 

  \item  \textbf{Predicate prediction}: We choose the head word $a_{pr}$ of event predicate as target word and the role \texttt{PRD} as target role. Then we consider all other word-role pairs of event participants as event context $e$. The likelihood function is computed using output from Equation \eqref{eq:softmax-w} as:
  \begin{equation} \label{eq:likelihood-pr}
  \begin{aligned}
  \mathcal{L}^{(pr)}(\theta, e) = p(a_{pr} | e, \texttt{PRD}). 
  \end{aligned}
  \end{equation}

  \item  \textbf{Semantic role prediction}: We choose the $l$-th  head word which is not empty word. We use its semantic role as target role $b_l$ and the head word as target word $a_l$. Then we consider all other word-role pairs as event context $e$. The likelihood function is computed using output from Equation \eqref{eq:softmax-r} as:
  \begin{equation} \label{eq:likelihood-r}
  \begin{aligned}
  \mathcal{L}^{(r)}(\theta, e) = \prod_{i = 1}^{L} p(b_l | e, a_l)
  \end{aligned}
  \end{equation}
where $L$ is the number of all non-empty head words in the event. 
\end{itemize}
Considering all three likelihood functions in a multi-task style, we define the likelihood function as:
\begin{equation} \label{eq:likelihood}
\begin{aligned}
    \mathcal{L}(\theta, e)
        &= \mathcal{L}^{(pr)}(\theta, e) \mathcal{L}^{(pa)}(\theta, e) + \alpha \mathcal{L}^{(r)}(\theta, e)    \\
        &= \mathcal{L}^{(w)}(\theta, e) + \alpha \mathcal{L}^{(r)}(\theta, e)
\end{aligned}
\end{equation}
where $\theta$ representing all parameters in the model and $\alpha$ is the hyper-parameter for the weight of semantic role prediction task. $\mathcal{L}^{(w)}(\theta, e)$ is the head word likelihood and $\mathcal{L}^{(r)}(\theta, e)$ is the semantic role likelihood. 
The model is trained over all events in the training data to obtain parameters $\theta$ that maximize the likelihood function as:
\begin{equation} \label{eq:mle}
\begin{aligned}
    \theta
        &= \mathop{argmax} \limits_{\theta} \mathcal{L}(\theta, e),\ e \in D_{train}
\end{aligned}
\end{equation}

To perform maximum likelihood estimation, we process a fixed number of input entries, a batch, each time and use AdaGrad algorithm \citep{duchi2011adaptive} as optimiser to update parameters. The batch size $bs$ and the learning rate $lr$ are hyper-parameters. When a specific number of the event predicates and participants in training data are processed one time, the period is considered as an iteration. This number is the iteration size $is$. After each training iteration, the model is validated on the validation data. The validation loss is defined as: 
\begin{equation} \label{eq:validation}
\begin{aligned}
    \mathcal{L}_{validate}
        &= \frac{1}{|D_{validate}|} \sum_{e \in D_{validate}} (-\log \mathcal{L}(\theta, e))
\end{aligned}
\end{equation}
When the validation loss stop decreasing for $dl$ iterations, we decrease the learning rate by multiplying $lr$ with a factor of $dr$. If the validation loss stop decreasing for $es$ iterations, we consider the model has converged. We perform early stopping and start model testing. The model is tested on the test data. The test loss is computed on test data as:
\begin{equation} \label{eq:test-likelihood}
\begin{aligned}
    \mathcal{L}_{test}
        &= \frac{1}{|D_{test}|} \sum_{e \in D_{test}} (-\log \mathcal{L}(\theta, e))
\end{aligned}
\end{equation}



\subsection{Model Comparison} 
We perform a model comparison between NNRF and MTEP. Our hypothesis is that with the additional semantic role prediction task, MTEP can learn a better event embedding. We train the models with the same setting and then perform two evaluations.

\subsubsection{Model Implementation} \label{sec:mtep-implementation}
We trained and validated both models on the RW-eng corpus. The hyper-parameters for model training we use are listed in Table \ref{tab:hyper}. We firstly use same model hyper-parameter setting, shown in the first section, as \citet{tilk2016event} to make a fair comparison. We choose $1.0$ as the weight of semantic role prediction $\alpha$ for simplicity. Because the data scale is enormous, in order to speed-up the training process, we make use of all 12 GB video memory and set the batch size $bs$ to $4000$. Using this batch size, we perform feature selection on learning rate. We train MTEP on the first $10\%$ of training set for $5$ iterations and observe the perplexity on test data. We compare the values in the set of $\{0.05, 0.1, 0.15\}$. The setting of $0.05$ has the worst performance and the setting of $0.15$ causes gradient explosion from time to time. The results show that $0.1$ is the optimised setting. We set the iteration size $is$ to $1 \times 10^8$. Because it is approximately $\frac{1}{5}$ of the training data size. With this setting, we can now consider $5$ iterations as one epoch. We set the early stopping iterations $es$ to $5$, set the decrease learning rate iterations $dl$ to $3$ that is about half of $es$ and set decreasing factor $dr$ to $0.1$ according to the suggestion of \citet{he2016deep}. We define the minimum difference between two values $\epsilon$ as $1.0*10^{-3}$. So if the difference between the loss of current iteration and the loss of best iteration is no more than $1.0*10^{-3}$, we consider the loss is not decreasing. 

We implement all models using Keras 2.0.5 \citep{chollet2015keras} with Tensorflow 1.2 \citep{abadi2016tensorflow} as backend. And then we deploy each model as a Docker container on a high performance computer with one 48 cores Intel Xeon E7 CPU and 256 GB main memory. We use one Nvidia Geforce Titan X GPU with 12 GB video memory as neural network accelerator. 


\begin{table}[t]
\centering
\begin{tabular}{l|l|l}
\textbf{Hyper-parameter} &  \textbf{Description}    &   \textbf{Value}  \\ \hline
$k$         &   Embeddings dimensions   &   $256$       \\
$d$         &   Hidden layer dimensions &   $512$       \\
$k^{(w)}$   &   Word output dimensions  &   $512$       \\
$k^{(w)}$   &   Role output dimensions  &   $512$       \\  \hline
$\alpha$    &   Role prediction weight  &   $1.0$       \\
$bs$        &   Batch size              &   $4000$      \\
$lr$        &   Learning rate           &   $0.1$       \\
$is$        &   Iteration size          &   $1 \times 10^8$ \\
$dl$        &   Decrease $lr$ iterations    &   $3$     \\
$dr$        &   Decreasing factor       &   $0.1$       \\
$es$        &   Early stopping iterations   &   $5$     \\
\end{tabular}
\caption{\label{tab:hyper} Hyper-parameters for model training. Model hyper-parameters are in the first section. Training hyper-parameters are in the second section.}
\end{table}


\subsubsection{Evaluation: Semantic Role Sensitiveness} \label{sec:comp-srs}
In the first evaluation, we want to test whether the model is sensitive to semantic roles of event participants. A better model can distinguish between two event with same head words but different semantic roles. For each entry which has non-empty \texttt{ARG0} role and \texttt{ARG1} role in the test dataset from the test data of the RW-eng corpus, we extract a target event and construct a foil event for the second event. For example entry \eqref{eg:event-sim}, we have:
\begin{equation}
    (table, draw, eye), (eye, draw, table)
\end{equation}
This subset is named as \textbf{target-foil event (TFE)} dataset which has $500$ entries. We use the head words with \texttt{ARG0} role and \texttt{ARG1} role as input. Then we obtain the event embedding vector defined in equation  \eqref{eq:sum-comp} from two models. We compute the similarity of the target event and the foil event using cosine similarity of two event embedding vectors. Generally the bigger of the similarity between these two events we obtain, the more role sensitive model we have. For each model, the means of these two similarities are computed across TFE dataset. The results are in Table \ref{tab:foil-mtep}. The MTEP has a lower mean similarity between target event and foil event. We use two-tailed paired $t$-test on these two lists of similarity scores, $t=-31.844$, $p=2.760 \times 10^{-122}$. The result,  highly significant, indicates that MTEP can better differentiate these two kinds of events. 


\subsubsection{Evaluation: Event Similarity} \label{sec:comp-eventsim}
The second experiment is event similarity correlation task which tests the compositional process of predicate and participants in each event. This task determine how well it forms an event with clear meaning for ambiguous predicates. We use the sentence similarity comparison task proposed by \citet{grefenstette2015concrete} (second experiment in their paper). The dataset, named \textbf{Grefenstette13} here, is originated from \citet{grefenstette2011experimental} and reannotated by Tuckers in 2013. Each row in the dataset contains a participant ID, two sentences with semantically related predicates, a human evaluation score of their similarity from $1$ to $7$ and a \texttt{HIGH}/\texttt{LOW} tag indicating how similar are two sentences. An example entry is here:
\begin{equation} \label{eg:event-sim}
    participant1, (table, draw, eye), (table, attract, eye), 7, \texttt{HIGH}
\end{equation}
We treat two sentences as two events. Naturally we can consider the subject $table$ has a \texttt{ARG0} semantic role and the object $eye$ has a \texttt{ARG1} semantic role in the event. We extract the event embedding vector for the first and second event. The similarity score of two events is computed as cosine similarity. Then we compute Spearman's $\rho$ correlation coefficient between model scores and human judgement scores. The result in every iteration is reported as a line chart in Figure \ref{fig:NNRF-MTEP-GS13}. The correlation scores keep stable after $20$ iterations. MTEP has a higher score than NNRF at almost every iteration. Thus MTEP is better correlated with human judgements. This indicates that with the additional semantic role prediction task, MTEP learns a better embedding of events. 


\begin{table}[t]
\centering
\begin{tabular}{l||l|l}
    \textbf{Model}  &  sample size  &   $mean \pm std$\\ \hline
    \textbf{NNRF}   &  $500$    &   $0.74\pm 0.13$  \\
    \textbf{MTEP}   &  $500$    &   $0.64\pm 0.12$  \\
\end{tabular}
\caption{\label{tab:foil-mtep} Cosine similarity between target event and foil event. Sample sizes are listed in the second column. In the third column, each score is the mean cosine similarity and the standard deviation. }
\end{table}
% * <asayeed@mbl.ca> 2017-11-13T22:52:35.875Z:
% 
% Did I miss how many items there were in this dataset? Can we get an idea of the significance of this difference? (I realize that this is not always easy to calculate, but at least the dataset size and some sort of variances may be useful here.)
% 
% ^ <xhong@coli.uni-saarland.de> 2017-11-20T00:30:06.655Z:
% 
% I added the number of entries in the dataset, standard deviations and explanation of motivation.
%
% ^.


\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{NNRF-MTEP-GS13.png}
\caption{\label{fig:NNRF-MTEP-GS13} Event similarity evaluation by training iteration of NNRF and MTEP. $x$-axis is the number of training iterations. $y$-axis is the Spearman's rho between model scores and human judgements. }
\end{figure}



\section{Bag-of-Event-Participant Model} \label{sec:bop}


\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{BOP.png}
\caption{\label{fig:BOP} Model architecture of bag-of-predicates/participants.}
\end{figure}


\subsection{Composition Method for Event Embedding} \label{sec:composition}
When it comes to the construction of event representation, there are two cases: participant order is known or unknown. In order to build a model which is compatible with first-order logic representation where conjunction connective does not imply order, 
We focus on methods in the condition that participant order is not provided and leave the other case for future exploration.

In the NNRF model, embedding vectors of event participants are sum up together to represent the whole event. Nonetheless, this is only a simple method to composite event participants. Each participant has the same weight in the event. Additionally, means of parameters can shift in a large range the after the sum operation. How can we improve this? We need an exploration of composition methods. 

We separate all possible methods into two groups. The first group is \textbf{non-parametric methods} which have no extra parameters like addition, multiplication and concatenation of event participant vectors. Another group is \textbf{parametric methods} which need more parameters like recursive unit \citep{socher2013recursive}, recurrent unit \citet{mikolov2010recurrent}, or long-short-term-memory recurrent unit \citep{hochreiter1997LSTM}. So the NNRF is classified as non-parametric composition method. 

Multiplication is an effective composition method for distributional semantic models. Unfortunately, during optimisation of neural network models, multiplication often results in extreme values which will lead to overflow or underflow. Concatenation is widely used in language modelling. But the concatenated feature cannot be used directly. An extra feature extractor on top of the concatenation is needed, which turns it into a heavyweight parametric method. 
% * <asayeed@mbl.ca> 2017-11-13T22:54:15.219Z:
% 
% "distributed semantic models" you mean...you're not referring to SDDM here right?
% 
% ^ <xhong@coli.uni-saarland.de> 2017-11-16T13:13:34.064Z:
% 
% Sorry, it is a typo. It is changed to 'distributional memory models'. 
%
% ^ <asayeed@mbl.ca> 2017-11-16T21:57:26.535Z:
% 
% But multiplication *isn't* an effective composition model for DMs due to the sparsity and the tendency to make everything 0.  The whole problem is that no one's been able to make a purely multiplicative model scale, right?
%
% ^ <xhong@coli.uni-saarland.de> 2017-11-20T08:55:12.079Z:
% 
% This claim is actually from two references of Tilk paper. 
% In Lenci 2011, they used 'PRODUCT' method for composition and obtained their best model on Bicknell dataset.
% In Grefenstette 2015 paper we used for the baseline of event similarity, they obtained competitive performance on multiplicative method. 
% Maybe I can use the general term 'multiplicative method' instead? 
%
% ^ <xhong@coli.uni-saarland.de> 2017-11-29T11:32:14.107Z:
% 
% modified to a more general term :  'distributional semantic model'
%
% ^ <xhong@coli.uni-saarland.de> 2017-11-29T11:32:15.613Z.


\subsection{Mean Composition Method} \label{sec:mean-composition}
We propose the \textbf{mean} composition function which computes the mean of all the vectors of event participants to represent the event as one vector. Computing the mean can be considered as normalisation of participant representations within the event boundary, which can prevent possible overflow/underflow of weights of the hidden vector. 

Since it is a non-parametric method, we also want to add a weight before the mean composition. Instead of using a fully-connected layer, we apply the PReLU to each embedding vector of participants to obtain the hidden vector: 
\begin{equation} \label{eq:nonlinearity-bop}
\begin{aligned}
    \mathbf{h}_l
        &= PReLU_l(\mathbf{p}_l) \\
\end{aligned}
\end{equation}
So the parameters inside PReLU can also act as weights of each participant. Using the mean method, the embedding vectors are composed as:
\begin{equation} \label{eq:mean-comp-bop}
\begin{aligned}
    \mathbf{e}
        &= \frac{1}{L} \sum_{l=1}^{L} \mathbf{h}_{l} \\
\end{aligned}
\end{equation}
where $\frac{1}{L}$ is used to compute the empirical mean of the hidden vector. 

Based on the MTEP model, we replace the composition function with mean composition function. The model is named as \textbf{bag-of-predicates/participants (BOP)} described in Figure \ref{fig:BOP} where the mean composition function is highlighted in the red border rectangle. 


\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{BOPRes.png}
\caption{\label{fig:BOPRes} Model architecture of bag-of-predicates/participants with residual learning.}
\end{figure}
% * <asayeed@mbl.ca> 2017-11-13T22:55:19.201Z:
% 
% Would be worth it to maybe highlight somehow visually the differences between these models? As in the most salient feature that distinguishes them from each other/NNRF?
% 
% ^ <xhong@coli.uni-saarland.de> 2017-11-16T13:14:37.312Z:
% 
% I updated with newest figures.  Do you mean I should write them in the text?
% 
% ^ <asayeed@mbl.ca> 2017-11-16T21:58:28.689Z:
% 
% Might be worth pointing out what your highlights mean.  Clarity never hurt.
%
% ^.


\subsection{Residual Learning} \label{sec:residual}
All models presented above contain two factored tensors. It has been shown that training model with factored tensors is difficult \citep{sutskever2011generating, kiros2014multimodal}. Due to the fact that each tensor element is represented as a product of three parameters, gradient explosion or vanishing problem happens more likely during the training. To tackle the gradient explosion or vanishing problem in deep neural networks, \citet{he2016deep} proposed the \textit{deep residual learning} framework. They have the observation that in a way of constructing a deep model from a shallow model, one needs to add additional layers which are identity mapping over other layers copied from the shallow model. As a result, we can force the layers to fit a residual mapping. Formally, a residual block containing two hidden layers using $PReLU$ activation function can be written as: 
\begin{equation} \label{eq:res-block}
\begin{aligned}
    \mathbf{h}^{(1)}
        &= PReLU(\mathbf{x}\mathbf{W}^{(1)}) \\
    \mathbf{h}^{(2)}
        &= PReLU(\mathbf{h}^{(1)}\mathbf{W}^{(2)} + \mathbf{x}) \\
\end{aligned}
\end{equation}
Inspiring by this idea, we employ residual learning in our BOP model. For the event-participant embedding in equation \eqref{eq:rbe-nnrf}, there are two product operators, the Hadamard product and the dot product. We want to construct the residual block here to separate weight flow between two product operators. So we rewrite the event-participant embedding vector as: 
\begin{equation} \label{eq:rbe-bopres}
\begin{aligned}
    \mathbf{p}_l
        &= (\mathbf{w}_i \mathbf{A}_e \circ \mathbf{r}_j \mathbf{B}_e) \mathbf{C}_e \\
        &= \mathbf{r}_l \mathbf{C}_e \\
\end{aligned}
\end{equation}
where $\mathbf{r}_l$, the composition of word embedding and semantic role label embedding, is treated as the residual. The original weight of the event-participant embedding goes into non-linear layer as equation \eqref{eq:nonlinearity-bop} and then dot products the weight matrix $\mathbf{W}_h$, but the weight of the residual goes into the event embedding directly as:
\begin{equation} \label{eq:hidden-bopres}
\begin{aligned}
    & \mathbf{h}_l^{(1)}
        = PReLU_l(\mathbf{r}_l \mathbf{C}_e) \\
    & \mathbf{h}_{l}
        = \mathbf{h}_{l}^{(1)}\mathbf{W}_h + \mathbf{r}_l \\
\end{aligned}
\end{equation}
Then the model perform the mean composition as  equation \eqref{eq:mean-comp-bop}. Since we only have one non-linearity layer in BOP, so the second non-linearity layer in residual block is removed. This model is named as \textbf{residual bag-of-predicates/participants (ResBOP)} described in Figure \ref{fig:BOPRes} where the residual block is highlighted in the red border rectangle. This model is trained with the same method described in Section  \ref{sec:training}. 



 
\section{Experiments}  \label{sec:exp}
We compare our models to previous baseline model NNRF on three tasks we define in Section \ref{sec:tasks}. We intend to show that our models is capable of perform semantic role prediction, while the models have competitive performance on both predicate prediction and participant prediction. 


\subsection{Model Implementation} \label{sec:implementation}
To perform a fair comparison, we train the baseline model NNRF and our three models, together named as \textbf{prediction-based event-level (PBEL)} models, on the newest version of RW-eng corpus. We use the same hyper-parameter setting for BOP and ResBOP as that for MTEP. Models are implemented with the same method and environment described in Section \ref{sec:mtep-implementation}. We list four PBEL models and their characteristics here for convenience. Each model is trained for 25 iterations or until converged. 


\begin{table}[t]
\centering
\begin{tabular}{l||l|lll}
\textbf{Characteristics}    & \textbf{NNRF} & \textbf{MTEP} & \textbf{BOP}  & \textbf{ResBOP} \\ \hline
Event-participant embedding &   \checkmark  &   \checkmark  &   \checkmark  &   \checkmark  \\
Multi-task learning         &   \texttimes  &   \checkmark  &   \checkmark  &   \checkmark  \\
Mean composition method     &   \texttimes  &   \texttimes  &   \checkmark  &   \checkmark  \\
Residual learning           &   \texttimes  &   \texttimes  &   \texttimes  &   \checkmark  \\  \hline
Number of parameters        &   38.9m       &   64.7m       &   64.7m       &   64.5m           \\
\end{tabular}
\caption{\label{tab:models} Summary of model characteristics. The first model NNRF is the baseline model from \citet{tilk2016event} and the others are the models proposed in this paper}
\end{table}



\subsection{Semantic Role Prediction}  \label{sec:roleprediction}
We want to compare our three models on the semantic role prediction task. Firstly, we record the role prediction accuracy on validation data of each iteration during training. The higher the accuracy is, the better the model performs. We plot the accuracy of each model versus the number of iterations in two line charts. The results are presented in Figure \ref{fig:exp-Role-Accuracy}. The BOP has the highest accuracy on semantic role prediction task. The accuracy of ResBOP is about $0.1\%$ lower. The results show that residual block bring down the performance of semantic role modelling in a small range. These two models have better accuracy than MTEP which indicates that the mean composition method helps with semantic role modelling. 

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{exp-Role-Accuracy.png}
\caption{\label{fig:exp-Role-Accuracy} Results of semantic role prediction. Role perplexity of the models. Role accuracy of the models.}
\end{figure}

Secondly, we compute the precision, recall and F1 score of the models after training on validation set and test set. The results are presented in Table \ref{tab:eval-role}. We notice that the recall of \texttt{AM-LOC} is very low. 
To get a better insight on the prediction of different semantic roles, we perform a error analysis. We construct a confusion matrix for each semantic role. The confusion matrix is presented in Figure .

\begin{table}[t]
\centering
\begin{tabular}{l||llll}
\textbf{Role}   &   Precision   &   Recall &   F1   \\ \hline
\texttt{PRD}    &   99.6 &   100       &   99.8 \\
\texttt{ARG0}   &   89.2 &   83.9      &   83.9 \\
\texttt{ARG1}   &   92.1 &   96.8      &   92.7 \\
\texttt{ARG2}   &   77.2 &   66.7      &   71.5 \\
\texttt{AM-MNR} &   88.8 &   68.7      &   77.5 \\
\texttt{AM-LOC} &   77.3 &   46.6      &   56.7 \\
\texttt{AM-TMP} &   87.3 &   77.2      &   81.9 \\
\end{tabular}
\caption{\label{tab:eval-role} Results of ResBOP on semantic role prediction.}
\end{table}


\subsection{Participant/Predicate Prediction}  \label{sec:wordprediction}
We also want to compare our models and NNRF on the participant and predicate prediction tasks. First, we compare the per word perplexities (PPL) of the models on validation set during training. PPL is a standard metric for language modelling which can be computed as: 
\begin{equation} \label{eq:perplexity}
\begin{aligned}
    \mathcal{PPL}
        &= \exp \mathcal{L}
\end{aligned}
\end{equation}
The lower the perplexity is, the better the model performs on the validation or test data. 

We record PPL on the validation data after each training iteration then compute perplexity for each iteration. Then we plot the perplexity of each model versus the number of iterations in a line chart. The results are presented in Figure \ref{fig:exp-Word-Perplexity}. The NNRF has the lowest perplexity on predicate and participant prediction. Our models have larger number of parameters which need more training iterations. MTEP has a slightly larger perplexity than NNRF. BOP and ResBOP have even higher perplexity. Since perplexity is a corpus-related metric which is correlated with word frequencies, the results show that these two models is not suitable for frequency-based role-filler prediction tasks. We notice that the PPLs are comparable larger than the results from state-of-the-art language models. The main reason is that we ignore functional words and only use limited information to predict head words. 

\begin{figure}[p]
\centering
\includegraphics[width=0.8\textwidth]{exp-Word-Perplexity.png}
\caption{\label{fig:exp-Word-Perplexity} Per word perplexity of the models.}
\end{figure}

Next, we compute PPLs after training on the validation data and the test data. In order to test the model in a fine-grain separation of role-fillers, we perform an analysis on each role. Other than the overall perplexities, we compute perplexity of target words for each semantic role. The result is presented in Table \ref{tab:eval-perplexity-role}. 

\begin{table}[t]
\centering
\begin{tabular}{l||llll}
\textbf{Role}   &   NNRF   &   MTEP    &   BOP      &   ResBOP         \\ \hline
all (valid)     &   325&    328   &   344  &   355      \\  \hline
all (test)      &   60 &   21      &   60  &   ?       \\
\texttt{PRD}    &   49 &   46      &   52  &   156     \\
\texttt{ARG0}   &   49 &   46      &   52  &   1520    \\
\texttt{ARG1}   &   39 &   39      &   12  &   923     \\
\texttt{ARG2}   &   49 &   46      &   52  &   1187    \\
\texttt{AM-MNR} &   12 &   12      &   44  &   530     \\
\texttt{AM-LOC} &   51 &   12      &   49  &   750     \\
\texttt{AM-TMP} &   60 &   12      &   60  &   225     \\
\end{tabular}
\caption{\label{tab:eval-perplexity-role} Perplexity by target roles.}
\end{table}



\section{Evaluations} \label{sec:evaluation}
Although our models show competitive performance on these three tasks we defined, we also want to show that our models are correlated with human behaviours in language understanding. We evaluate our models on several human judgement correlation tasks. We obtain the model file of NNRF which trained in an old version of RW-eng corpus. To distinguish with the reimplementation of NNRF, this model is named \textbf{NNRF16}. 


\subsection{Human Thematic Fit Judgement Correlation}  \label{sec:thematicfit}


\begin{figure}[p]
\centering
\includegraphics[width=0.8\textwidth]{exp-Pado07.png}
\caption{\label{fig:exp-Pado07} Evaluation scores versus iterations in Pado07 dataset. }
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[width=0.8\textwidth]{exp-McRae05.png}
\caption{\label{fig:exp-McRae05} Evaluation scores versus iterations in McRae05 dataset. }
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[width=0.8\textwidth]{exp-F-Inst.png}
\caption{\label{fig:exp-F-Inst} Evaluation scores versus iterations in F-Inst dataset. }
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[width=0.8\textwidth]{exp-F-Loc.png}
\caption{\label{fig:exp-F-Loc} Evaluation scores versus iterations in F-Loc dataset. }
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[width=0.8\textwidth]{exp-GDS.png}
\caption{\label{fig:exp-GDS} Evaluation scores versus iterations in GDS dataset. }
\end{figure}

Human thematic fit judgement correlations (HTFJC) task is a method to evaluate the correlation between semantic representations and human cognition in the aspect of predicate-participant composition \citep{sayeed2016thematic}. Thematic fit is to consider whether an event participant fulfils the selection preference of a predicate given a thematic role. For example, in sentence \ref{eg:thematic}, how well \textit{knife} fulfils the patient role of the predicate \textit{cut}. Previous works show that human judgements of thematic fit map well to underlying cognitive process of human \citep{pado2009probabilistic,vandekerckhove2009robust}. This task can also capture human biases that predicate-participant composition with low frequency can also have high human judgement score. Thus it reveals the cognitive plausibility of models on the event predicate-participant structure of events. By evaluating our models on this task, we can show that our model emits a better probability distribution for cognitive modelling and learns good representations of event predicate-participant compositions. 

\subsubsection{Datasets} \label{sec:dataset-thematic}
Most of the datasets are from cognitive experiments regarding to thematic fit. The general structure of an entry in datasets in thematic fit is like: 
\begin{equation} \label{eg:data-thematic}
    accept, friend, \texttt{AGENT}, 6.1
\end{equation}
where \textit{accept} is the predicate, \textit{friend} is the role-filler, \texttt{AGENT} is the target thematic role and $6.1$ is the score of human judgement in a scale from $1$ (least common) to $7$ (most common). The score is usually a mean value of several experiment participants. We use the following datasets chosen from \citet{sayeed2016thematic}: 
\begin{itemize}
\item \textbf{Pado07}       the dataset proposed by \citet{pado2007integration} with 414 entries with a format of general format. The role is either \texttt{AGENT} or \texttt{PATIENT}. The predicates are selected according to their frequencies in the Penn TreeBank and FrameNet. The role-fillers are chosen to have a bigger variance of scores within each predicate. 
\item   \textbf{McRae05}    the dataset from \citet{mcrae2005basis} containing 1444 entries in general format. The role is either \texttt{AGENT} or \texttt{PATIENT}. This dataset is a combination of data from 5 previous experiments which have significant overlap in stimuli but slightly different standards on word selection. This is the largest dataset in thematic fit judgement to our knowledge. 
\item   \textbf{Ferretti01} the dataset proposed by  \citet{ferretti2001integrating} has two subset. One subset (\textbf{F-Inst}) contains 248 entries of ratings for instrument role. Another subset (\textbf{F-Loc}) contains 274 entries of ratings for location role. All the entries are in general format. 
\item   \textbf{GDS}    the dataset from \citet{greenberg2015verb} with 720 entries on patient ratings in general format. There are two subsets in this data. The first one, named as \textbf{G-Filler}, with 240 entries is adopted from \citet{mcrae1998modeling} but with new human judgements. The second one, named as \textbf{G-Object}, with 480 entries contains half of entries with monosemy verbs and another half with polysemy verbs. They designed a different judgement elicitation question to avoid the frequency effect causing by the term \textit{common/typical} in previous design of question. The dataset is constructed to vary word frequency and verb polysemy systematically. 
\end{itemize}


\subsubsection{Methods and Results}  \label{sec:method-thematic}
For four PBEL models, since the role labels of event participants are not thematic role but PropBank style semantic roles, we have to map the thematic roles to semantic roles. Under the semantic role model we define in Section \ref{sec:repr}, we can define the mapping as: (\texttt{AGENT}->\texttt{ARG0}), (\texttt{PATIENT}->\texttt{ARG1}), (\texttt{LOCATION}->\texttt{AM-LOC}), (\texttt{INSTRUMENT}->\texttt{AM-MNR}). 

For each entry in the dataset, we use the predicate as input word, \texttt{PRD} as input role and the semantic role from mapping as target role. Then we compute the output probability of participant prediction task defined in Equation \eqref{eq:softmax-nnrf} in each model. Because the models are trained using maximum likelihood estimation, this probability is correlated to target word frequency in the training data. However, \citet{greenberg2015verb} has shown that human thematic fit judgement is not sensitive to noun frequency. \citet{tilk2016event} pointed out that the output bias of a PBEL model is strongly correlated with word frequency in training data. As a matter of fact, to reduce the frequency effect, we adopt the probability adjustment from  that sets the bias of output layer in Equation \eqref{eq:output-nnrf} to zero during the evaluation. We record evaluation scores in each dataset and plot the scores versus training iterations. 

The results are in Figure \ref{fig:exp-Pado07}, \ref{fig:exp-McRae05}, \ref{fig:exp-F-Loc}, \ref{fig:exp-F-Inst}, \ref{fig:exp-GDS}. On Pado07 and McRae05, NNRF and MTEP have similar performance since they have similar architecture on participant prediction. Our new model BOP has higher score than those two models. This indicates that the mean composition method actually helps with high frequency roles like agent and patient. ResBOP have the highest performance during training. Especially on Pado07 dataset, the difference between ResBOP and the model with second highest score is quite big. This is because predicates in Pado07 are only those with highest frequency. So the other other models can easily overfit on these dataset but the residual block in ResBOP reduce the gradient explosion effect on high frequency samples which leads to better results. On F-Inst, ResBOP has the highest score. But there are no significant differences of the models. On F-Loc, NNRF surprising has the highest score then MTEP follows. This possiblely reveals that for the location role, both additional task and mean composition method are burdens. On GDS, NNRF also has the highest score and MTEP follows. Since there are three subset in this dataset, we perform more in-depth analysis in Section \ref{sec:result-thematic-ap}. Overall, we notice that the results of HTFJC task is not correlated with word perplexity. We believe that this is because word perplexity is frequency-correlated but human judgement is not. Although we try to prune out the frequency effect by setting output bias to zero, the models learn better features for cognitive modelling task can still have high perplexities.



\subsubsection{Comparison with Baselines} \label{sec:result-thematic}
In addition to PBEL models, we want to compare with previous count-based distributional semantic models. The most successful method on thematic fit has been prototype filler construction. Given a triple of verb-noun-role as input, the prototype filler vector is constructed by computing the mean of vectors of role-fillers which have top statistic scores from the model. The thematic fit score is generally computed as the cosine similarity between the prototype filler vector and the target word vector. The baseline models are listed here: 
\begin{itemize}
  \item \textbf{TypeDM: }   This is best distributional memory model from \citet{baroni2010distributional} described in Section \ref{sec:dsm}.  The dimensionality of the sparse vectors is about 750-million. The prototype is constructed as the mean of 20 role-fillers with top LMI scores from the tensor. The results of this model is from a reimplementation in \citet{greenberg2015improving}. 
  \item \textbf{SDDM: } The idea of this model is originated from \citet{sayeed2014combining} described in Section \ref{sec:dsm}. The dimensionality of the sparse vectors is about 1-million. \citet{sayeed2015exploration} performed a comparison between variants of SDDM on head-finding heuristics. The best model (\textbf{SDDM-mo}) which use only Malt parser for head-finding with balanced vocabulary has overall best performance. An extended variant (\textbf{SDDMX}) is proposed by \citet{greenberg2015improving}. The model has additional links between head nouns belongs to the same event. The links are labelled with verb lemma for the 400 most frequent verbal predicates. 
  \item \textbf{SDS15-avg: }    This is a simple ensemble model from \citet{sayeed2015exploration}. The thematic fit score is the average of results from SDDM and TypeDM. 
  \item \textbf{SDS15-swap: }   This is another ensemble model from \citet{sayeed2015exploration}. TypeDM is used to select top ranking role-fillers. Then the corresponding vectors are extracted from SDDM to compute the representation of their prototype filler. The thematic score is computed as cosine similarity in the SDDM vector space. 
  \item \textbf{GSD15: }    This is the overall best-performing model from \citet{greenberg2015improving} described in Section \ref{sec:dsm}. A different prototype construction method is applied on TypeDM, which use hierarchical clustering algorithm to cluster typical role-fillers regarding to verb senses given the input role. 
  \item \textbf{SCLB17: }   This is a model from \citet{santus2017measuring} described in Section \ref{sec:dsm}. We report the  best-performing model on F-Inst here because it is the state-of-the-art on instrument role. The number of the fillers is 30, the number of top features $N$ is 2000 and we only report highest results among different types of dependency contexts. 
\end{itemize}

The results are in Table \ref{tab:eval-thematic}. For the same model with different implementation, the results could vary. We therefore only report the highest result across all references. If there is no specific annotation, the result is from the original paper which proposed the model. On Pado07, SDDM-mo is still the current state-of-the-art model. SDS15-avg obtains the highest performance among unsupervised systems on Pado07 dataset but it is an ensemble model. On F-Loc, our reimplementation of NNRF outperform other models in a wide range and remains to be the state-of-the-art model. Our three new models obtain competitive performance. Our best model is the state-of-the-art model on McRae05 and F-Inst dataset. 

\begin{table}[t]
\centering
\begin{tabular}{l||lllllll}
\textbf{Model} & Pado07       & McRae05     & F-Loc     & F-Inst        &   G-Object    &   G-Filler    &   all     \\ \hline 
TypeDM  &   53      &   33      &   23      &   36          &   53*         &       31*     &   35      \\ 
SDDM-mo &   56      &   27      &   13      &   28          &   41          &       16      &   31      \\ 
GSD15   &   50      &   36      &   29      &   42          &   55                  &       30      &   39      \\
NNRF16  &   52      &   38      &   45      &   45          &   65          & \textbf{47}   &   41      \\  
SCLB17  &   49      &   28      &   37      &   50          &   -           &       -       &   -       \\  \hline 
NNRF    &   48      &   39      &\textbf{52}&   50          &   xx          & \textbf{xx}   &   xx      \\
MTEP    &   49      &   39      &   48      &   50          &   xx          &       xx      &   xx      \\
BOP     &   52      & \textbf{41} & 45      &   51          &   \textbf{66} &       37      &   xx      \\
ResBOP  &   55      & \textbf{41} & 45      &   \textbf{52} &   65          &       42      &   xx      \\  \hline  
SDS15-swap& 48      &   25      &   19      &   45          &   50          &       29      &   33      \\  
SDS15-avg&\textbf{59}&  34      &   21      &   39          &   51          &       26      &   38      \\
\end{tabular}
\caption{\label{tab:eval-thematic} Results on human thematic fit judgement correlation task comparing to previous works. Some results of TypeDM are taken from \citet{greenberg2015verb}(*). $p\text{-value} < 0.05$ for all results. }
\end{table}


\subsubsection{Analysis on Agent and Patient} \label{sec:result-thematic-ap}
In previous works, agent and patient are the most well-studied thematic roles. Because it can be clearly defined as proto-agent and proto-patient for most of transitive predicates like we mentioned in Section \ref{sec:semanticrole}. In order to obtain more insights on differences of these models, we focus on three datasets with these two roles. 
For this purpose, we add several more baseline models from the other works: 
\begin{itemize}
  \item \textbf{CW: }       This model, named \textit{Collobert and Weston}, is the 100-dimensional pre-trained embedding vectors from SENNA \citep{collobert2011natural}. The model is trained on the Wikipedia corpus. The result is taken from \citet{baroni2014don}. Comparing with this model, we want to see whether our model can retrieve better features though we use semantic role labels from SENNA. 
  \item \textbf{BDK-pre: }  This is the best \textit{predict} model from \citet{baroni2014don}. It is a CBOW model, described in \ref{sec:repr}, trained on UBW corpus. The prototype is selected using top role-fillers from a query to TypeDM. 
  \item \textbf{BagPack: }  This is the only supervised model here proposed by \citet{herdagdelen2009bagpack}. This can be considered as an upper bound of all unsupervised methods. 
\end{itemize}
Firstly, we separate Pado07 dataset into two subsets. \textbf{P-Agen} is the subset with \texttt{AGENT} role and \textbf{P-Pati} is the subset with \texttt{PATIENT} role. We compute the Spearman's $\rho$ separately for each subset. The results are in Table \ref{tab:eval-thematic-pado07}. 
\begin{table}[t]
\centering
\begin{tabular}{l||llll}
\textbf{Model}  &   Pado07 &   P-Agen   &   P-Pati \\ \hline
TypeDM      &   53      &\textbf{54}&   53          \\
SDDMX       &   52      &   53      &   51          \\
BDK-cnt     &   41      &   -       &   -           \\
GDS15       &   50      &   46      &   56          \\  
SDS15-avg   &   59      &   -       &   -           \\
SDS15-swap  &   48      &   -       &   -           \\\hline
CW          &   28      &   -       &   -           \\
BDK-pre     &   41      &   -       &   -           \\
NNRF16      &   52      &   49      &   \textbf{59} \\
NNRF        &   47      &   42      &   53          \\
MTEP        &   48      &   39      &   57          \\
BOP         &   52      &   46      &   \textbf{59} \\
ResBOP      &   55      &   51      &   58          \\  \hline
BagPack     &\textbf{60}&   -       &   -           \\
\end{tabular}
\caption{\label{tab:eval-thematic-pado07} Fine-grain evaluation result of models on Pado07 datasets. '-' means the value is not available.  $p\text{-value} < 0.05$ for all results. }
\end{table}


\begin{table}[t]
\centering
\begin{tabular}{l||llll}
\textbf{Model}  &   McRae &   M-Agen   &   M-Pati   \\ \hline
TypeDM      &   53      &\textbf{54}&   53          \\
SDDM-mo     &   52      &   53      &   51          \\
SDDMX       &   52      &   53      &   51          \\
GDS15       &   50      &   46      &   56          \\  \hline
NNRF16      &   52      &   49      &\textbf{59}    \\
NNRF        &   47      &   42      &   53          \\
MTEP        &   48      &   39      &   57          \\
BOP         &   52      &   48      &   56          \\
ResBOP      &   55      &   51      &   58          \\
\end{tabular}
\caption{\label{tab:eval-thematic-mcrae05} Fine-grain evaluation result of models on McRae05 datasets. '-' means the value is not available. $p\text{-value} < 0.05$ for all results. }
\end{table}


\subsection{Compositionality}  \label{sec:compo}
The datasets in HTFJC task only have judgements of thematic fit activated by predicates of events. However as we mentioned in \ref{sec:event}, other event participants appeared in the event can affect human expectation of the upcoming role-filler. We want to test whether our models can model the compositional process between the predicate and the participant. Due to the limitation of existing evaluation datasets, we make a hypothesis that our models are effective in two forms of compositionality: 1) agent and predicate; 2) agent, predicate and patient. We prove this hypothesis via two evaluations. 


\subsubsection{Agent-Predicate} \label{sec:result-bicknell}
In the first case, we use a small dataset, named as \textbf{Bicknell10}, from \citet{bicknell2010effects} with 128 rows. Each row in the dataset contains a sentence in a format like (agent, predicate, patient), a \texttt{YES}/\texttt{NO} tag indicating the gold standard judgement of whether the patient is typical given the predicate and the agent. Each two rows with the same agent and predicate are considered as a pair with one \texttt{YES} tag and one \texttt{NO} tag. There are 64 pairs in total. An example pair of two rows is like: 
\begin{equation*} \label{eg:event-sim}
\begin{aligned}
    &(secretary, address, letter),  \texttt{YES} \\
    &(secretary, address, group),   \texttt{NO}  
\end{aligned}
\end{equation*}
The task for the model is to choose the \texttt{YES}/\texttt{NO} tags on each pair. The naive baseline model, named \textbf{Random}, is to choose the tags uniformly random. \citet{LenciECU} proposed a baseline model, named \textbf{Lenci11}, which is a compositional model based on TypeDM. We use the \textit{Accuracy 1} metric proposed in \citet{tilk2016event}. This metric counts a hit if and only if the model assigns the composed agent-predicate a higher score when we test a patient with \texttt{YES} tag in contrast the other patient with \texttt{NO} tag. 

For PBEL models, we perform this evaluation after each training iteration and record the result. Then we plot the results versus number of iterations. The result is in Figure \ref{fig:exp-Bicknell10}. The results of other models vibrate in a wide range. This is due to the fact that the data size is too small. The ResBOP is the best-performing model of most training iteration. MTEP has lower score than NNRF at almost every iteration, which reveals that with the sum composition method, the role prediction task can harm the effectiveness of agent-predicate compostionality. But this problem is fixed by the residual block in ResBOP. To compare with previous baselines, we list all the results in Table \ref{tab:Bicknell10}. The ResBOP has the highest accuracy comparing to previous works. 

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{exp-Bicknell10.png}
\caption{\label{fig:exp-Bicknell10} Results of compositionality.}
\end{figure}

\begin{table*}
\centering
\begin{tabular}{l|lll|llll}
\textbf{Model}      & Random & Lenci11 & NNRF16 & NNRF & MTEP &   BOP    &   ResBOP         \\ \hline \hline
\textbf{Accuracy 1} & 0.500  & 0.672   & 0.687  & 0.702& 0.733&   0.733  & \textbf{0.765}   \\
\end{tabular}
\caption{\label{tab:Bicknell10} Results on agent-patient compositionality evaluation comparing to previous models. $p\text{-value} < 0.05$ for all results. }
\end{table*}


\subsubsection{Agent-Predicate-Patient} \label{sec:result-eventsim}
To further test the compositional process of event participants, we evaluate our models in the event similarity correlation task we described in Section \ref{sec:comp-eventsim}. The baseline models are listed here: 
\begin{itemize}
  \item \textbf{Add: } An additive model proposed by \citet{mitchell2008vector}. The event representation is composed by the sum of vectors of predicate and participants . 
  \item \textbf{Multiply: } An multiplicative model proposed by \citet{mitchell2008vector}. The event representation is composed by the element-wise product of vectors of predicate and participants. 
  \item \textbf{Kronker: }  A model proposed by \citet{grefenstette2015concrete} using Kronecker product as composition method. 
  \item \textbf{CBOW: }     The CBOW model implemented by \citet{tilk2016event} trained on an old version of RW-eng corpus.
\end{itemize}
We choose the result at the $25$-th iteration as the final result and compare these two systems with previous state-of-the-art models. The result is in Table \ref{tab:GS}. The MTEP obtain state-of-the-art performance. 

\begin{table}[t]
\centering
\begin{tabular}{l||l}
\textbf{Model}  &   \textbf{$\rho \times 100$}  \\ \hline
Add         &   10    \\
Multiply    &   16    \\
Kronecker   &   26    \\  \hline
CBOW        &   13    \\
NNRF16      &   34    \\  \hline
NNRF        &   32    \\
MTEP        &   37    \\
BOP         &   34    \\
ResBOP      &   34    \\  \hline
Human       &   60    \\
\end{tabular}
\caption{\label{tab:GS} Results on event similarity evaluation comparing to previous models. $p\text{-value} < 0.05$ for all results. }
\end{table}



\subsection{Word Similarity Correlation Task}  \label{sec:wordsim}
% * <asayeed@mbl.ca> 2017-11-13T22:59:17.281Z:
% 
% The meaning of this needs quite a bit more in-context explanation I think.  But I suppose that's why it's at the end here :)
% 
% ^.
\subsubsection{Datasets} \label{sec:dataset-wordsim}

\subsubsection{Baselines} \label{sec:baselines-wordsim}
The models are listed here: 
\begin{itemize}
  \item \textbf{GloVe: }    \citet{pennington2014glove}.
  \item \textbf{CBOW: }     t
  \item \textbf{skipgram: } t
  \item \textbf{MTEP: }     t
  \item \textbf{BOP: }      t
  \item \textbf{SOA: }      t
\end{itemize}


\subsubsection{Results} \label{sec:result-wordsim}
The results are in Table \ref{tab:wordsim}. 
\begin{table}[t]
\centering
\begin{tabular}{l||l|l|l|l}
\textbf{Model}  &   Adj.&   Noun    &   Verb    &   all \\ \hline
GloVe       &       57  &   38      &   16      &   35  \\
CBOW        &       58  &   48      &   25      &   43  \\
skip-gram   &       60  &   44      &   31      &   46  \\ \hline
MTEP        &       29  & \textbf{51} & 39      &   45  \\
BOP         &       40  &   50      &   39      &   46  \\ \hline
SOA         & \textbf{66} & 50  & \textbf{58} & \textbf{56} \\
Human       &       -   &   -       &   -       &   67  \\
\end{tabular}
\caption{\label{tab:wordsim} Word similarity evaluation on SimLex999 comparing to previous models. Each score is Spearman's $\rho \times 100$. $p\text{-value} < 0.05$ for all results. SOA is the state-of-the-art model. }
\end{table}





\section{Related Works}
The second model proposed by \citet{tilk2016event}, \textit{incremental model}, is a parametric method. A recurrent unit is added to express participant order not only within event but also across different events. Moreover, a binary indicator $b$ is used to detect the event boundaries. $b$ equals to $1$ if the target word belongs to current event, otherwise $0$. So the event embedding vector is: 
\begin{equation} \label{eq:incremental}
\begin{aligned}
    \mathbf{e} 
        &= \mathbf{p} + \mathbf{h_{t-1}}\mathbf{W}_h + b\mathbf{v}, 
\end{aligned}
\end{equation}
where $\mathbf{v}$ is the parameter vector of event boundary. $\mathbf{h_{t-1}}$ is the hidden vector of last time step in the recurrent unit. $\mathbf{W}_h$ is the parameter matrix of recurrent unit. 


\section{Future Works}
This model can be employed in incremental semantic parsing \citep{konstas2014incremental, konstas2015semantic} or inferences of logical forms (TODO: add ref). 

In \textit{neo-Davidsonian event representation}, we notice that linguistic predicates can be further generalised. So we define a special role \texttt{PRD} for predicates. The example \ref{eg:neodavidsonian} can be rewritten as: 
\begin{equation*} \label{eg:symbolic-thematic}
\begin{aligned}
    \exists e\ \texttt{PRD}(e, \text{cut})
    & \land \texttt{AGENT}(e, \text{cook}) \land \texttt{PATIENT}(e, \text{cake}) \\
    & \land \texttt{INSTRUMENT}(e, \text{knife}) \land \texttt{LOCATION}(e, \text{kitchen}) \\
\end{aligned}
\end{equation*}

If we specify the sense of the predicate $cut$, thematic roles can be replaced with PropBank style semantic roles as: 
\begin{equation*} \label{eg:symbolic-semantic}
\begin{aligned}
    \exists e\ \texttt{PRD}(e, \text{cut.01})
    & \land \texttt{ARG0}(e, \text{cook}) \land \texttt{ARG1}(e, \text{cake}) \\
    & \land \texttt{ARGM-MNR}(e, \text{knife}) \land \texttt{ARGM-LOC}(e, \text{kitchen}) \\
\end{aligned}
\end{equation*}
This is a representation of the event in first order logic. 


\section{Conclusion} 
\subsection{Summary of This Paper} 






 

\newpage
\bibliographystyle{acl_natbib}
\bibliography{mtrf}



\end{document}